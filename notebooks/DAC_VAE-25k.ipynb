{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df12d45c-a97d-4b67-a6ae-e6f4f35d3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUDA DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.1\n",
      "PyTorch Version: 2.5.1+cu121\n",
      "Number of GPUs: 1\n",
      "\n",
      "--- GPU 0 ---\n",
      "Name: NVIDIA RTX A1000\n",
      "Capability: (8, 6)\n",
      "Total Memory: 8.00 GB\n",
      "Available Memory: 7.03 GB\n",
      "Allocated Memory: 0.00 GB\n",
      "✓ Can create tensors on GPU 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CUDA DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"\\nCUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\n--- GPU {i} ---\")\n",
    "        print(f\"Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        \n",
    "        # Memory info\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Available Memory: {torch.cuda.mem_get_info(i)[0] / 1024**3:.2f} GB\")\n",
    "        print(f\"Allocated Memory: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        \n",
    "        # Test tensor creation\n",
    "        try:\n",
    "            test_tensor = torch.randn(100, 100).cuda(i)\n",
    "            print(f\"✓ Can create tensors on GPU {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error creating tensor: {e}\")\n",
    "else:\n",
    "    print(\"\\n✗ CUDA is NOT available!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15bf932-78c2-492a-b0b2-08664e9c0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 7.03 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Kill all CUDA processes\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reset peak memory stats\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "print(f\"Available Memory: {torch.cuda.mem_get_info(0)[0] / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13a9abc-a3db-4b11-a6d9-4ab0a5d6d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: einops in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: descript-audio-codec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: torch==2.5.1+cu121 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchaudio) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1+cu121->torchaudio) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1+cu121->torchaudio) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1+cu121->torchaudio) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1+cu121->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1+cu121->torchaudio) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1+cu121->torchaudio) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1+cu121->torchaudio) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1+cu121->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: argbind>=0.3.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audio-codec) (0.3.9)\n",
      "Requirement already satisfied: descript-audiotools>=0.7.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audio-codec) (0.7.2)\n",
      "Requirement already satisfied: docstring-parser in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from argbind>=0.3.7->descript-audio-codec) (0.17.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.13.1)\n",
      "Requirement already satisfied: pyloudnorm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.1.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (6.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (1.16.2)\n",
      "Requirement already satisfied: julius in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.2.7)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (1.0.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (9.6.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (14.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (3.10.7)\n",
      "Requirement already satisfied: librosa in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.11.0)\n",
      "Requirement already satisfied: pystoi in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.4.1)\n",
      "Requirement already satisfied: torch-stoi in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.2.3)\n",
      "Requirement already satisfied: flatten-dict in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.4.2)\n",
      "Requirement already satisfied: markdown2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (2.5.4)\n",
      "Requirement already satisfied: randomname in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.2.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (3.19.6)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (2.20.0)\n",
      "Requirement already satisfied: six<2.0,>=1.12 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flatten-dict->descript-audiotools>=0.7.2->descript-audio-codec) (1.17.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jedi>=0.16->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.8.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.5.1+cu121->torchaudio) (3.0.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.62.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.5.2)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.1.2)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba>=0.51.0->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (4.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.1.0->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from soundfile->descript-audiotools>=0.7.2->descript-audio-codec) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.0->soundfile->descript-audiotools>=0.7.2->descript-audio-codec) (2.23)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (2.9.0.post0)\n",
      "Requirement already satisfied: future>=0.16.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyloudnorm->descript-audiotools>=0.7.2->descript-audio-codec) (1.0.0)\n",
      "Requirement already satisfied: fire in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from randomname->descript-audiotools>=0.7.2->descript-audio-codec) (0.7.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fire->randomname->descript-audiotools>=0.7.2->descript-audio-codec) (3.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->descript-audiotools>=0.7.2->descript-audio-codec) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->descript-audiotools>=0.7.2->descript-audio-codec) (0.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Core dependencies\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "!pip install torchaudio transformers einops tqdm descript-audio-codec\n",
    "#audiotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c97d916-76f7-4190-9f75-0157220a915d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting descript-audio-codec\n",
      "  Using cached descript_audio_codec-1.0.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting argbind>=0.3.7 (from descript-audio-codec)\n",
      "  Downloading argbind-0.3.9.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting descript-audiotools>=0.7.2 (from descript-audio-codec)\n",
      "  Downloading descript_audiotools-0.7.2-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: einops in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audio-codec) (0.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audio-codec) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audio-codec) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audio-codec) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audio-codec) (4.67.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from argbind>=0.3.7->descript-audio-codec) (6.0.3)\n",
      "Collecting docstring-parser (from argbind>=0.3.7->descript-audio-codec)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: soundfile in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.13.1)\n",
      "Collecting pyloudnorm (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting importlib-resources (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (1.16.2)\n",
      "Collecting julius (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ffmpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (1.0.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (9.6.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (14.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (3.10.7)\n",
      "Requirement already satisfied: librosa in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (0.11.0)\n",
      "Collecting pystoi (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting torch-stoi (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading torch_stoi-0.2.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting flatten-dict (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting markdown2 (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading markdown2-2.5.4-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting randomname (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading randomname-0.2.1.tar.gz (64 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting protobuf<3.20,>=3.9.2 (from descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading protobuf-3.19.6-py2.py3-none-any.whl.metadata (828 bytes)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from descript-audiotools>=0.7.2->descript-audio-codec) (2.20.0)\n",
      "Requirement already satisfied: six<2.0,>=1.12 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flatten-dict->descript-audiotools>=0.7.2->descript-audio-codec) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython->descript-audiotools>=0.7.2->descript-audio-codec) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jedi>=0.16->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.8.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->descript-audio-codec) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->descript-audio-codec) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->descript-audio-codec) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->descript-audio-codec) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->descript-audio-codec) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->descript-audio-codec) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->descript-audio-codec) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch->descript-audio-codec) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->descript-audio-codec) (3.0.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.62.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.5.2)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa->descript-audiotools>=0.7.2->descript-audio-codec) (1.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lazy_loader>=0.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba>=0.51.0->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (4.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (2025.10.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.1.0->librosa->descript-audiotools>=0.7.2->descript-audio-codec) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from soundfile->descript-audiotools>=0.7.2->descript-audio-codec) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.0->soundfile->descript-audiotools>=0.7.2->descript-audio-codec) (2.23)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->descript-audiotools>=0.7.2->descript-audio-codec) (2.9.0.post0)\n",
      "Collecting future>=0.16.0 (from pyloudnorm->descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting fire (from randomname->descript-audiotools>=0.7.2->descript-audio-codec)\n",
      "  Using cached fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fire->randomname->descript-audiotools>=0.7.2->descript-audio-codec) (3.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->descript-audiotools>=0.7.2->descript-audio-codec) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->descript-audiotools>=0.7.2->descript-audio-codec) (0.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython->descript-audiotools>=0.7.2->descript-audio-codec) (0.2.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard->descript-audiotools>=0.7.2->descript-audio-codec) (3.1.3)\n",
      "Downloading descript_audio_codec-1.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading descript_audiotools-0.7.2-py2.py3-none-any.whl (106 kB)\n",
      "Downloading protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading markdown2-2.5.4-py3-none-any.whl (49 kB)\n",
      "Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Downloading torch_stoi-0.2.3-py3-none-any.whl (8.1 kB)\n",
      "Building wheels for collected packages: argbind, julius, randomname\n",
      "  Building wheel for argbind (pyproject.toml): started\n",
      "  Building wheel for argbind (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for argbind: filename=argbind-0.3.9-py2.py3-none-any.whl size=11896 sha256=c0578537454cc2e67847e087da9aea6ead5537e8fc84c2955723743d0b3b822e\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\eb\\33\\cb\\c5e898b01c657604d61ef1462002dee37ed67b4b05b871dc45\n",
      "  Building wheel for julius (pyproject.toml): started\n",
      "  Building wheel for julius (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=22017 sha256=2d924306cf804f5d409961173fca17ad58e1ee21d4b15e58ddfca67ba2df8e5b\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\de\\c1\\ca\\544dafe48401e8e2e17064dfe465a390fca9e8720ffa12e744\n",
      "  Building wheel for randomname (pyproject.toml): started\n",
      "  Building wheel for randomname (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for randomname: filename=randomname-0.2.1-py3-none-any.whl size=89313 sha256=ae09ca85a6b768f7bd46a456a9c6a9565dc2ca77f7ceae32aae7aa2d8a41b7a6\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\05\\6e\\37\\1db72aaa77a57f2dcbc97fd96a47f14091130a1dde562124c0\n",
      "Successfully built argbind julius randomname\n",
      "Installing collected packages: protobuf, markdown2, importlib-resources, future, flatten-dict, fire, docstring-parser, randomname, pystoi, pyloudnorm, argbind, julius, torch-stoi, descript-audiotools, descript-audio-codec\n",
      "\n",
      "  Attempting uninstall: protobuf\n",
      "\n",
      "    Found existing installation: protobuf 6.33.0\n",
      "\n",
      "    Uninstalling protobuf-6.33.0:\n",
      "\n",
      "      Successfully uninstalled protobuf-6.33.0\n",
      "\n",
      "   ----------------------------------------  0/15 [protobuf]\n",
      "   ----------------------------------------  0/15 [protobuf]\n",
      "   ----- ----------------------------------  2/15 [importlib-resources]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   -------- -------------------------------  3/15 [future]\n",
      "   ------------- --------------------------  5/15 [fire]\n",
      "   ------------- --------------------------  5/15 [fire]\n",
      "   ---------------- -----------------------  6/15 [docstring-parser]\n",
      "   ------------------ ---------------------  7/15 [randomname]\n",
      "   --------------------- ------------------  8/15 [pystoi]\n",
      "   -------------------------------- ------- 12/15 [torch-stoi]\n",
      "   ---------------------------------- ----- 13/15 [descript-audiotools]\n",
      "   ------------------------------------- -- 14/15 [descript-audio-codec]\n",
      "   ---------------------------------------- 15/15 [descript-audio-codec]\n",
      "\n",
      "Successfully installed argbind-0.3.9 descript-audio-codec-1.0.0 descript-audiotools-0.7.2 docstring-parser-0.17.0 fire-0.7.1 flatten-dict-0.4.2 future-1.0.0 importlib-resources-6.5.2 julius-0.2.7 markdown2-2.5.4 protobuf-3.19.6 pyloudnorm-0.1.1 pystoi-0.4.1 randomname-0.2.1 torch-stoi-0.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "databricks-sdk 0.70.0 requires protobuf<7.0,>=4.21.0, but you have protobuf 3.19.6 which is incompatible.\n",
      "opentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.19.6 which is incompatible.\n",
      "ray 2.52.1 requires protobuf>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow 2.20.0 requires protobuf>=5.28.0, but you have protobuf 3.19.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/descriptinc/audiotools\n",
      "  Cloning https://github.com/descriptinc/audiotools to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-5_d7i_qg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/descriptinc/audiotools 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-5_d7i_qg'\n",
      "  fatal: unable to access 'https://github.com/descriptinc/audiotools/': Could not resolve host: github.com\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  git clone --filter=blob:none --quiet https://github.com/descriptinc/audiotools 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-5_d7i_qg' did not run successfully.\n",
      "  exit code: 128\n",
      "  \n",
      "  No available output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "ERROR: Failed to build 'git+https://github.com/descriptinc/audiotools' when git clone --filter=blob:none --quiet https://github.com/descriptinc/audiotools 'c:\\users\\user\\appdata\\local\\temp\\pip-req-build-5_d7i_qg'\n"
     ]
    }
   ],
   "source": [
    "!pip install --retries 10 --timeout 30 descript-audio-codec\n",
    "!pip install --retries 10 --timeout 30 git+https://github.com/descriptinc/audiotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aacc4de9-e0cc-40af-9e54-61cd9184b346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\user\\.cache\\dac\\protobuf-5.28.3-cp310-abi3-win_amd64.whl\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "Successfully installed protobuf-5.28.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "descript-audiotools 0.7.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.28.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# In notebook:\n",
    "!pip install \"C:/Users/user/.cache/dac/protobuf-5.28.3-cp310-abi3-win_amd64.whl\" --force-reinstal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c46124-1a6a-4d59-894c-92e9d55fd70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DAC model found! You can train!\n",
      "   Location: C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\n",
      "   Size: 245.08 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dac_model_path = \"C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\"\n",
    "\n",
    "if os.path.exists(dac_model_path):\n",
    "    print(\"✅ DAC model found! You can train!\")\n",
    "    print(f\"   Location: {dac_model_path}\")\n",
    "    file_size = os.path.getsize(dac_model_path) / (1024 * 1024)\n",
    "    print(f\"   Size: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"❌ DAC model NOT found!\")\n",
    "    print(f\"   Expected location: {dac_model_path}\")\n",
    "    print(\"\\n📥 You need to download it first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cc5674-8aca-41fc-ae75-07624bd30a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DAC library imported successfully\n",
      "============================================================\n",
      "DAC-VAE AUDIO EFFECT GENERATOR (No AudioTools)\n",
      "============================================================\n",
      "Device: cuda\n",
      "Sample Rate: 44100 Hz\n",
      "Max Length: 5.0 seconds\n",
      "Batch Size: 2 x 4 = 8\n",
      "============================================================\n",
      "\n",
      "Loading DAC model...\n",
      "✓ Loading from: C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\audiotools\\ml\\layers\\base.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dict = torch.load(location, \"cpu\")\n",
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DAC model loaded and frozen\n",
      "✓ DAC Latent Channels: 128\n",
      "✓ Time Reduction Factor: 512x\n",
      "\n",
      "============================================================\n",
      "LOADING DATASET\n",
      "============================================================\n",
      "Original dataset: 25000 samples\n",
      "\n",
      "Validating files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████| 25000/25000 [00:06<00:00, 4115.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Valid samples: 25000\n",
      "\n",
      "Dataset splits:\n",
      "  Train:      17500 samples (70.0%)\n",
      "  Validation: 3750 samples (15.0%)\n",
      "  Test:       3750 samples (15.0%)\n",
      "\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8A8B5F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11002] getaddrinfo failed)\"))'), '(Request ID: 1eaf8a62-4700-493b-bf8d-92991e403feb)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B01460>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: ed94bf2f-a9ce-46cf-844e-b4de4eb102b1)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B11F40>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: a9aa399d-ecca-4bf6-9675-9b9182b8b13c)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B12630>: Failed to resolve \\'huggingface.co\\' ([Errno 11002] getaddrinfo failed)\"))'), '(Request ID: 4260e7c9-1304-4fa8-b13b-9eaf65d3b3c6)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B13E30>: Failed to resolve \\'huggingface.co\\' ([Errno 11002] getaddrinfo failed)\"))'), '(Request ID: fe8559d4-d8e4-4335-8ad6-26a913f7fa67)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B03260>: Failed to resolve \\'huggingface.co\\' ([Errno 11002] getaddrinfo failed)\"))'), '(Request ID: 1cbf82bc-5d29-444e-a4de-b0f7d2f1adff)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B2AC90>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2eda0766-8239-4fb3-8782-03e75c4dff4e)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded\n",
      "\n",
      "============================================================\n",
      "CREATING DATALOADERS\n",
      "============================================================\n",
      "Batches per epoch:\n",
      "  Train: 8750 batches\n",
      "  Val:   1875 batches\n",
      "  Test:  1875 batches\n",
      "\n",
      "============================================================\n",
      "INITIALIZING MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B2A990>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 60d11b4c-5c67-4be4-af26-b682444cb9f1)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000168F8B29FD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 61e1e76d-02ff-4a1f-bb09-6737f949d356)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoder: FROZEN ❄️\n",
      "Total parameters: 188,877,378\n",
      "Trainable parameters: 15,181,888\n",
      "Frozen parameters: 173,695,490\n",
      "UNet channels: [64, 128, 256, 512]\n",
      "Latent channels: 128\n",
      "\n",
      "============================================================\n",
      "TRAINING SETUP\n",
      "============================================================\n",
      "Optimizer: AdamW\n",
      "Learning rate: 1e-05\n",
      "Scheduler: CosineAnnealingLR\n",
      "Loss: L1 (audio) + MSE (latent)\n",
      "Mixed precision: True\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Total epochs: 40\n",
      "Steps per epoch: 8750\n",
      "Validation every epoch\n",
      "============================================================\n",
      "\n",
      "Loading checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23812\\3221969232.py:875: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(cfg.checkpoint_path, map_location=cfg.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Resumed from epoch 38\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 39/40\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   1%|▏             | 100/8750 [01:08<1:37:01,  1.49it/s, loss=0.7671, audio=0.1776, latent=5.8943, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 100/8750 | Loss: 0.655026 | Audio: 0.106148 | Latent: 5.488788 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   2%|▎             | 200/8750 [02:10<1:27:16,  1.63it/s, loss=0.5594, audio=0.0421, latent=5.1729, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 200/8750 | Loss: 0.656523 | Audio: 0.105389 | Latent: 5.511334 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   3%|▍             | 300/8750 [03:11<1:29:13,  1.58it/s, loss=0.6196, audio=0.0283, latent=5.9131, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 300/8750 | Loss: 0.657436 | Audio: 0.104366 | Latent: 5.530697 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   5%|▋             | 400/8750 [04:14<1:24:09,  1.65it/s, loss=0.5588, audio=0.0700, latent=4.8876, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 400/8750 | Loss: 0.652667 | Audio: 0.101991 | Latent: 5.506758 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   6%|▊             | 500/8750 [05:14<1:26:21,  1.59it/s, loss=0.5925, audio=0.0515, latent=5.4102, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 500/8750 | Loss: 0.652828 | Audio: 0.101911 | Latent: 5.509167 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   7%|▉             | 600/8750 [06:14<1:19:28,  1.71it/s, loss=0.7269, audio=0.1790, latent=5.4783, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 600/8750 | Loss: 0.652258 | Audio: 0.101260 | Latent: 5.509978 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   8%|█             | 700/8750 [07:13<1:19:58,  1.68it/s, loss=0.5393, audio=0.0296, latent=5.0973, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 700/8750 | Loss: 0.652127 | Audio: 0.100551 | Latent: 5.515763 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:   9%|█▎            | 800/8750 [08:13<1:17:39,  1.71it/s, loss=0.7872, audio=0.1783, latent=6.0891, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 800/8750 | Loss: 0.651316 | Audio: 0.100004 | Latent: 5.513127 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  10%|█▍            | 900/8750 [09:12<1:17:43,  1.68it/s, loss=0.7988, audio=0.1784, latent=6.2033, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 900/8750 | Loss: 0.653674 | Audio: 0.102161 | Latent: 5.515129 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  11%|█▍           | 1000/8750 [10:11<1:15:51,  1.70it/s, loss=0.6036, audio=0.0526, latent=5.5102, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1000/8750 | Loss: 0.654521 | Audio: 0.102428 | Latent: 5.520933 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  13%|█▋           | 1100/8750 [11:11<1:18:39,  1.62it/s, loss=0.5451, audio=0.0489, latent=4.9621, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1100/8750 | Loss: 0.653945 | Audio: 0.101856 | Latent: 5.520893 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  14%|█▊           | 1200/8750 [12:11<1:15:51,  1.66it/s, loss=0.7470, audio=0.1975, latent=5.4954, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1200/8750 | Loss: 0.653423 | Audio: 0.101453 | Latent: 5.519700 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  15%|█▉           | 1300/8750 [13:11<1:14:43,  1.66it/s, loss=0.7169, audio=0.1818, latent=5.3508, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1300/8750 | Loss: 0.653155 | Audio: 0.100954 | Latent: 5.522008 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  16%|██           | 1400/8750 [14:10<1:15:27,  1.62it/s, loss=0.9020, audio=0.3248, latent=5.7717, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1400/8750 | Loss: 0.653110 | Audio: 0.101203 | Latent: 5.519077 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  17%|██▏          | 1500/8750 [15:10<1:10:29,  1.71it/s, loss=0.7754, audio=0.1832, latent=5.9219, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1500/8750 | Loss: 0.652877 | Audio: 0.100768 | Latent: 5.521086 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  18%|██▍          | 1600/8750 [16:09<1:11:02,  1.68it/s, loss=0.5148, audio=0.0420, latent=4.7275, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1600/8750 | Loss: 0.652647 | Audio: 0.100801 | Latent: 5.518460 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  19%|██▌          | 1700/8750 [17:08<1:09:00,  1.70it/s, loss=0.5169, audio=0.0459, latent=4.7099, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1700/8750 | Loss: 0.652809 | Audio: 0.101159 | Latent: 5.516495 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  21%|██▋          | 1800/8750 [18:08<1:10:37,  1.64it/s, loss=0.5215, audio=0.0679, latent=4.5352, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1800/8750 | Loss: 0.652968 | Audio: 0.101363 | Latent: 5.516049 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  22%|██▊          | 1900/8750 [19:07<1:09:23,  1.65it/s, loss=0.5877, audio=0.0565, latent=5.3121, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1900/8750 | Loss: 0.652446 | Audio: 0.100841 | Latent: 5.516050 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  23%|██▉          | 2000/8750 [20:06<1:09:02,  1.63it/s, loss=0.5228, audio=0.0677, latent=4.5513, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2000/8750 | Loss: 0.652670 | Audio: 0.101080 | Latent: 5.515900 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  24%|███          | 2100/8750 [21:04<1:03:39,  1.74it/s, loss=0.5676, audio=0.0252, latent=5.4244, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2100/8750 | Loss: 0.652782 | Audio: 0.101024 | Latent: 5.517577 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  25%|███▎         | 2200/8750 [22:03<1:04:08,  1.70it/s, loss=0.7712, audio=0.1790, latent=5.9223, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2200/8750 | Loss: 0.652988 | Audio: 0.100948 | Latent: 5.520391 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  26%|███▍         | 2300/8750 [23:02<1:04:15,  1.67it/s, loss=0.7361, audio=0.1987, latent=5.3741, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2300/8750 | Loss: 0.653831 | Audio: 0.101343 | Latent: 5.524871 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  27%|███▌         | 2400/8750 [24:00<1:03:36,  1.66it/s, loss=0.7120, audio=0.1842, latent=5.2775, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2400/8750 | Loss: 0.653901 | Audio: 0.101469 | Latent: 5.524324 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  29%|████▎          | 2500/8750 [24:59<59:53,  1.74it/s, loss=0.7327, audio=0.1774, latent=5.5526, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2500/8750 | Loss: 0.654104 | Audio: 0.101858 | Latent: 5.522460 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  30%|███▊         | 2600/8750 [25:58<1:01:07,  1.68it/s, loss=0.6703, audio=0.1780, latent=4.9227, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2600/8750 | Loss: 0.654196 | Audio: 0.101872 | Latent: 5.523232 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  31%|████▋          | 2700/8750 [26:56<58:37,  1.72it/s, loss=0.6076, audio=0.0332, latent=5.7443, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2700/8750 | Loss: 0.654321 | Audio: 0.102231 | Latent: 5.520900 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  32%|████▊          | 2800/8750 [27:55<59:26,  1.67it/s, loss=0.7176, audio=0.1764, latent=5.4124, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2800/8750 | Loss: 0.653927 | Audio: 0.101985 | Latent: 5.519415 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  33%|████▉          | 2900/8750 [28:54<56:12,  1.73it/s, loss=0.7521, audio=0.1873, latent=5.6485, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2900/8750 | Loss: 0.654036 | Audio: 0.102323 | Latent: 5.517124 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  34%|█████▏         | 3000/8750 [29:53<56:14,  1.70it/s, loss=0.5797, audio=0.0514, latent=5.2838, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3000/8750 | Loss: 0.653022 | Audio: 0.101614 | Latent: 5.514079 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  35%|█████▎         | 3100/8750 [30:53<58:13,  1.62it/s, loss=0.8975, audio=0.3238, latent=5.7372, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3100/8750 | Loss: 0.652935 | Audio: 0.101606 | Latent: 5.513285 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  37%|█████▍         | 3200/8750 [31:52<55:08,  1.68it/s, loss=0.5995, audio=0.0498, latent=5.4970, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3200/8750 | Loss: 0.653315 | Audio: 0.101935 | Latent: 5.513801 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  38%|█████▋         | 3300/8750 [32:51<52:06,  1.74it/s, loss=0.5811, audio=0.0561, latent=5.2500, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3300/8750 | Loss: 0.652990 | Audio: 0.101667 | Latent: 5.513232 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  39%|█████▊         | 3400/8750 [33:50<52:00,  1.71it/s, loss=0.5316, audio=0.0425, latent=4.8912, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3400/8750 | Loss: 0.653040 | Audio: 0.101699 | Latent: 5.513408 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  40%|██████         | 3500/8750 [34:49<51:50,  1.69it/s, loss=0.5593, audio=0.0387, latent=5.2067, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3500/8750 | Loss: 0.653262 | Audio: 0.102081 | Latent: 5.511813 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  41%|██████▏        | 3600/8750 [35:47<49:14,  1.74it/s, loss=0.5744, audio=0.0312, latent=5.4316, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3600/8750 | Loss: 0.653498 | Audio: 0.102290 | Latent: 5.512084 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  42%|██████▎        | 3700/8750 [36:47<48:44,  1.73it/s, loss=0.5672, audio=0.0473, latent=5.1986, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3700/8750 | Loss: 0.653478 | Audio: 0.102208 | Latent: 5.512696 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  43%|██████▌        | 3800/8750 [37:45<47:33,  1.73it/s, loss=0.7452, audio=0.1958, latent=5.4934, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3800/8750 | Loss: 0.653112 | Audio: 0.102156 | Latent: 5.509560 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  45%|██████▋        | 3900/8750 [38:44<48:19,  1.67it/s, loss=0.7642, audio=0.1832, latent=5.8106, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3900/8750 | Loss: 0.653315 | Audio: 0.102317 | Latent: 5.509985 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  46%|██████▊        | 4000/8750 [39:43<47:02,  1.68it/s, loss=0.5490, audio=0.0581, latent=4.9092, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4000/8750 | Loss: 0.653214 | Audio: 0.102249 | Latent: 5.509643 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  47%|███████        | 4100/8750 [40:42<45:40,  1.70it/s, loss=0.7184, audio=0.1761, latent=5.4234, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4100/8750 | Loss: 0.653237 | Audio: 0.102384 | Latent: 5.508531 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  48%|███████▏       | 4200/8750 [41:42<45:14,  1.68it/s, loss=0.5691, audio=0.0360, latent=5.3309, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4200/8750 | Loss: 0.652895 | Audio: 0.102184 | Latent: 5.507107 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  49%|███████▎       | 4300/8750 [42:41<43:42,  1.70it/s, loss=0.5718, audio=0.0381, latent=5.3369, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4300/8750 | Loss: 0.652961 | Audio: 0.102235 | Latent: 5.507266 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  50%|███████▌       | 4400/8750 [43:40<44:27,  1.63it/s, loss=0.8870, audio=0.3220, latent=5.6507, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4400/8750 | Loss: 0.652639 | Audio: 0.102097 | Latent: 5.505414 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  51%|███████▋       | 4500/8750 [44:39<42:03,  1.68it/s, loss=0.5399, audio=0.0437, latent=4.9617, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4500/8750 | Loss: 0.652413 | Audio: 0.101764 | Latent: 5.506485 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  53%|███████▉       | 4600/8750 [45:38<41:27,  1.67it/s, loss=0.9546, audio=0.3244, latent=6.3024, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4600/8750 | Loss: 0.652529 | Audio: 0.101769 | Latent: 5.507606 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  54%|████████       | 4700/8750 [46:38<38:16,  1.76it/s, loss=0.5280, audio=0.0482, latent=4.7978, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4700/8750 | Loss: 0.652616 | Audio: 0.101918 | Latent: 5.506973 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  55%|████████▏      | 4800/8750 [47:36<38:48,  1.70it/s, loss=0.9574, audio=0.3295, latent=6.2793, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4800/8750 | Loss: 0.652638 | Audio: 0.101930 | Latent: 5.507077 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  56%|████████▍      | 4900/8750 [48:34<37:15,  1.72it/s, loss=0.6062, audio=0.0274, latent=5.7884, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4900/8750 | Loss: 0.652792 | Audio: 0.102100 | Latent: 5.506915 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  57%|████████▌      | 5000/8750 [49:33<36:32,  1.71it/s, loss=0.7182, audio=0.1958, latent=5.2241, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5000/8750 | Loss: 0.652889 | Audio: 0.101910 | Latent: 5.509787 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  58%|████████▋      | 5100/8750 [50:32<36:52,  1.65it/s, loss=0.7356, audio=0.1786, latent=5.5704, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5100/8750 | Loss: 0.652646 | Audio: 0.101751 | Latent: 5.508952 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  59%|████████▉      | 5200/8750 [51:30<34:47,  1.70it/s, loss=0.5614, audio=0.0450, latent=5.1643, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5200/8750 | Loss: 0.652553 | Audio: 0.101741 | Latent: 5.508117 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  61%|█████████      | 5300/8750 [52:28<33:28,  1.72it/s, loss=0.5814, audio=0.0612, latent=5.2018, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5300/8750 | Loss: 0.652203 | Audio: 0.101526 | Latent: 5.506768 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  62%|█████████▎     | 5400/8750 [53:27<32:18,  1.73it/s, loss=0.5793, audio=0.0468, latent=5.3246, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5400/8750 | Loss: 0.651905 | Audio: 0.101373 | Latent: 5.505326 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  63%|█████████▍     | 5500/8750 [54:25<32:54,  1.65it/s, loss=0.5598, audio=0.0503, latent=5.0958, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5500/8750 | Loss: 0.652012 | Audio: 0.101501 | Latent: 5.505115 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  64%|█████████▌     | 5600/8750 [55:25<31:44,  1.65it/s, loss=0.6501, audio=0.0642, latent=5.8586, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5600/8750 | Loss: 0.652263 | Audio: 0.101794 | Latent: 5.504692 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  65%|█████████▊     | 5700/8750 [56:24<30:00,  1.69it/s, loss=0.6931, audio=0.0596, latent=6.3346, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5700/8750 | Loss: 0.652315 | Audio: 0.101914 | Latent: 5.504004 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  66%|█████████▉     | 5800/8750 [57:23<29:48,  1.65it/s, loss=0.7033, audio=0.1977, latent=5.0564, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5800/8750 | Loss: 0.652535 | Audio: 0.102189 | Latent: 5.503468 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  67%|██████████     | 5900/8750 [58:22<27:53,  1.70it/s, loss=0.7358, audio=0.2005, latent=5.3527, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5900/8750 | Loss: 0.652367 | Audio: 0.102040 | Latent: 5.503263 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  69%|██████████▎    | 6000/8750 [59:21<26:23,  1.74it/s, loss=0.5787, audio=0.0229, latent=5.5573, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6000/8750 | Loss: 0.652278 | Audio: 0.102077 | Latent: 5.502009 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  70%|█████████    | 6100/8750 [1:00:20<27:00,  1.64it/s, loss=0.5289, audio=0.0545, latent=4.7441, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6100/8750 | Loss: 0.652181 | Audio: 0.101975 | Latent: 5.502059 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  71%|█████████▏   | 6200/8750 [1:01:20<24:57,  1.70it/s, loss=0.7106, audio=0.1747, latent=5.3593, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6200/8750 | Loss: 0.651888 | Audio: 0.101791 | Latent: 5.500974 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  72%|█████████▎   | 6300/8750 [1:02:18<24:05,  1.69it/s, loss=0.5397, audio=0.0454, latent=4.9423, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6300/8750 | Loss: 0.652101 | Audio: 0.101961 | Latent: 5.501400 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  73%|█████████▌   | 6400/8750 [1:03:16<22:25,  1.75it/s, loss=0.6301, audio=0.0622, latent=5.6789, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6400/8750 | Loss: 0.651835 | Audio: 0.101850 | Latent: 5.499856 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  74%|█████████▋   | 6500/8750 [1:04:15<22:11,  1.69it/s, loss=0.5533, audio=0.0604, latent=4.9290, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6500/8750 | Loss: 0.651704 | Audio: 0.101737 | Latent: 5.499673 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  75%|█████████▊   | 6600/8750 [1:05:14<21:05,  1.70it/s, loss=0.7153, audio=0.1686, latent=5.4670, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6600/8750 | Loss: 0.651767 | Audio: 0.101877 | Latent: 5.498901 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  77%|█████████▉   | 6700/8750 [1:06:13<20:04,  1.70it/s, loss=0.6127, audio=0.0527, latent=5.5993, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6700/8750 | Loss: 0.651827 | Audio: 0.102009 | Latent: 5.498177 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  78%|██████████   | 6800/8750 [1:07:12<19:46,  1.64it/s, loss=0.7358, audio=0.1724, latent=5.6337, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6800/8750 | Loss: 0.651728 | Audio: 0.101948 | Latent: 5.497803 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  79%|██████████▎  | 6900/8750 [1:08:10<18:09,  1.70it/s, loss=0.5600, audio=0.0497, latent=5.1032, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6900/8750 | Loss: 0.651657 | Audio: 0.101934 | Latent: 5.497222 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  80%|██████████▍  | 7000/8750 [1:09:09<17:14,  1.69it/s, loss=0.8211, audio=0.2054, latent=6.1576, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7000/8750 | Loss: 0.651798 | Audio: 0.101945 | Latent: 5.498526 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  81%|██████████▌  | 7100/8750 [1:10:09<16:57,  1.62it/s, loss=0.6028, audio=0.0553, latent=5.4748, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7100/8750 | Loss: 0.651731 | Audio: 0.101828 | Latent: 5.499027 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  82%|██████████▋  | 7200/8750 [1:11:08<15:38,  1.65it/s, loss=0.8741, audio=0.3186, latent=5.5550, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7200/8750 | Loss: 0.651572 | Audio: 0.101748 | Latent: 5.498243 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  83%|██████████▊  | 7300/8750 [1:12:07<13:49,  1.75it/s, loss=0.5000, audio=0.0380, latent=4.6198, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7300/8750 | Loss: 0.651473 | Audio: 0.101769 | Latent: 5.497045 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  85%|██████████▉  | 7400/8750 [1:13:06<13:25,  1.68it/s, loss=0.5715, audio=0.0502, latent=5.2132, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7400/8750 | Loss: 0.651414 | Audio: 0.101699 | Latent: 5.497146 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  86%|███████████▏ | 7500/8750 [1:14:05<12:13,  1.70it/s, loss=0.6028, audio=0.0526, latent=5.5013, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7500/8750 | Loss: 0.651715 | Audio: 0.101929 | Latent: 5.497863 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  87%|███████████▎ | 7600/8750 [1:15:04<11:37,  1.65it/s, loss=0.5796, audio=0.0331, latent=5.4646, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7600/8750 | Loss: 0.651525 | Audio: 0.101824 | Latent: 5.497003 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  88%|███████████▍ | 7700/8750 [1:16:04<10:47,  1.62it/s, loss=0.7307, audio=0.1782, latent=5.5246, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7700/8750 | Loss: 0.651410 | Audio: 0.101831 | Latent: 5.495794 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  89%|███████████▌ | 7800/8750 [1:17:02<09:22,  1.69it/s, loss=0.5690, audio=0.0610, latent=5.0804, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7800/8750 | Loss: 0.651346 | Audio: 0.101773 | Latent: 5.495735 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  90%|███████████▋ | 7900/8750 [1:18:01<08:14,  1.72it/s, loss=0.5588, audio=0.0475, latent=5.1137, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7900/8750 | Loss: 0.651219 | Audio: 0.101733 | Latent: 5.494860 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  91%|███████████▉ | 8000/8750 [1:19:00<07:28,  1.67it/s, loss=0.5878, audio=0.0624, latent=5.2541, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8000/8750 | Loss: 0.651167 | Audio: 0.101656 | Latent: 5.495109 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  93%|████████████ | 8100/8750 [1:19:58<06:34,  1.65it/s, loss=0.5549, audio=0.0270, latent=5.2787, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8100/8750 | Loss: 0.651131 | Audio: 0.101676 | Latent: 5.494555 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  94%|████████████▏| 8200/8750 [1:20:57<05:38,  1.63it/s, loss=0.5545, audio=0.0596, latent=4.9490, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8200/8750 | Loss: 0.651235 | Audio: 0.101843 | Latent: 5.493918 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  95%|████████████▎| 8300/8750 [1:21:55<04:21,  1.72it/s, loss=0.8987, audio=0.3211, latent=5.7763, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8300/8750 | Loss: 0.651072 | Audio: 0.101771 | Latent: 5.493010 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  96%|████████████▍| 8400/8750 [1:22:54<03:21,  1.74it/s, loss=0.5483, audio=0.0591, latent=4.8925, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8400/8750 | Loss: 0.650930 | Audio: 0.101750 | Latent: 5.491792 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  97%|████████████▋| 8500/8750 [1:23:53<02:25,  1.72it/s, loss=0.7595, audio=0.1782, latent=5.8126, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8500/8750 | Loss: 0.650823 | Audio: 0.101605 | Latent: 5.492177 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  98%|████████████▊| 8600/8750 [1:24:52<01:29,  1.68it/s, loss=0.8053, audio=0.2110, latent=5.9424, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8600/8750 | Loss: 0.650627 | Audio: 0.101554 | Latent: 5.490725 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40:  99%|████████████▉| 8700/8750 [1:25:51<00:30,  1.66it/s, loss=0.5681, audio=0.0490, latent=5.1908, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8700/8750 | Loss: 0.650490 | Audio: 0.101520 | Latent: 5.489699 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|█████████████| 8750/8750 [1:26:21<00:00,  1.69it/s, loss=0.5986, audio=0.0396, latent=5.5898, nans=0]\n",
      "Validation 39/40: 100%|██████████████████| 1875/1875 [16:47<00:00,  1.86it/s, loss=0.5529, audio=0.0460, latent=5.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EPOCH 39/40 SUMMARY\n",
      "============================================================\n",
      "Train Loss:  0.650385 (Audio: 0.101473, Latent: 5.489111)\n",
      "Val Loss:    0.643559 (Audio: 0.097770, Latent: 5.457897)\n",
      "Learning Rate: 8.75e-06\n",
      "============================================================\n",
      "\n",
      "✅ NEW BEST MODEL! Val Loss: 0.643559\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 40/40\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   1%|▏             | 100/8750 [00:57<1:25:30,  1.69it/s, loss=0.7062, audio=0.1866, latent=5.1965, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 100/8750 | Loss: 0.649064 | Audio: 0.101185 | Latent: 5.478794 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   2%|▎             | 200/8750 [01:58<1:24:01,  1.70it/s, loss=0.5297, audio=0.0359, latent=4.9375, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 200/8750 | Loss: 0.641960 | Audio: 0.096821 | Latent: 5.451392 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   3%|▍             | 300/8750 [03:03<1:35:52,  1.47it/s, loss=0.7228, audio=0.1771, latent=5.4571, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 300/8750 | Loss: 0.639453 | Audio: 0.095745 | Latent: 5.437082 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   5%|▋             | 400/8750 [04:01<1:20:54,  1.72it/s, loss=0.6113, audio=0.0257, latent=5.8562, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 400/8750 | Loss: 0.641338 | Audio: 0.096941 | Latent: 5.443973 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   6%|▊             | 500/8750 [05:00<1:17:41,  1.77it/s, loss=0.5995, audio=0.0280, latent=5.7150, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 500/8750 | Loss: 0.642157 | Audio: 0.097914 | Latent: 5.442430 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   7%|▉             | 600/8750 [05:59<1:20:53,  1.68it/s, loss=0.5298, audio=0.0415, latent=4.8825, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 600/8750 | Loss: 0.640996 | Audio: 0.097496 | Latent: 5.434998 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   8%|█             | 700/8750 [06:58<1:16:01,  1.76it/s, loss=0.5738, audio=0.0385, latent=5.3535, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 700/8750 | Loss: 0.644864 | Audio: 0.100261 | Latent: 5.446033 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:   9%|█▎            | 800/8750 [07:56<1:18:01,  1.70it/s, loss=0.5024, audio=0.0295, latent=4.7292, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 800/8750 | Loss: 0.645266 | Audio: 0.101273 | Latent: 5.439930 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  10%|█▍            | 900/8750 [08:53<1:14:19,  1.76it/s, loss=0.5812, audio=0.0201, latent=5.6114, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 900/8750 | Loss: 0.646832 | Audio: 0.102601 | Latent: 5.442305 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  11%|█▍           | 1000/8750 [09:51<1:19:11,  1.63it/s, loss=0.5976, audio=0.0544, latent=5.4321, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1000/8750 | Loss: 0.645366 | Audio: 0.101114 | Latent: 5.442514 | NaNs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  12%|█▌           | 1040/8750 [10:14<1:14:14,  1.73it/s, loss=0.4805, audio=0.0393, latent=4.4115, nans=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Bad gradient (norm=inf) at step 1039, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  13%|█▋           | 1100/8750 [10:49<1:13:59,  1.72it/s, loss=0.4866, audio=0.0489, latent=4.3766, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1100/8750 | Loss: 0.650426 | Audio: 0.101399 | Latent: 5.490264 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  14%|█▊           | 1200/8750 [12:13<2:06:39,  1.01s/it, loss=0.5814, audio=0.0349, latent=5.4646, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1200/8750 | Loss: 0.649992 | Audio: 0.101241 | Latent: 5.487503 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  15%|█▉           | 1300/8750 [13:51<1:53:31,  1.09it/s, loss=0.9052, audio=0.3232, latent=5.8205, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1300/8750 | Loss: 0.649253 | Audio: 0.100377 | Latent: 5.488765 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  16%|██           | 1400/8750 [15:31<2:03:44,  1.01s/it, loss=0.7773, audio=0.1845, latent=5.9281, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1400/8750 | Loss: 0.648717 | Audio: 0.100598 | Latent: 5.481191 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  17%|██▏          | 1500/8750 [17:12<2:02:15,  1.01s/it, loss=0.7519, audio=0.1837, latent=5.6826, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1500/8750 | Loss: 0.648452 | Audio: 0.100709 | Latent: 5.477429 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  18%|██▍          | 1600/8750 [18:53<2:01:22,  1.02s/it, loss=0.5548, audio=0.0408, latent=5.1401, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1600/8750 | Loss: 0.648931 | Audio: 0.101533 | Latent: 5.473980 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  19%|██▌          | 1700/8750 [20:34<1:59:41,  1.02s/it, loss=0.6010, audio=0.0345, latent=5.6649, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1700/8750 | Loss: 0.649303 | Audio: 0.101541 | Latent: 5.477622 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  21%|██▋          | 1800/8750 [22:15<1:57:56,  1.02s/it, loss=0.7162, audio=0.1818, latent=5.3438, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1800/8750 | Loss: 0.648685 | Audio: 0.101094 | Latent: 5.475916 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  22%|██▊          | 1900/8750 [23:56<1:54:14,  1.00s/it, loss=0.7390, audio=0.1844, latent=5.5459, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 1900/8750 | Loss: 0.648435 | Audio: 0.100999 | Latent: 5.474361 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  23%|██▉          | 2000/8750 [25:36<1:54:28,  1.02s/it, loss=0.6292, audio=0.0372, latent=5.9202, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2000/8750 | Loss: 0.647979 | Audio: 0.100828 | Latent: 5.471507 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  24%|███          | 2100/8750 [27:17<1:52:20,  1.01s/it, loss=0.5996, audio=0.0233, latent=5.7627, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2100/8750 | Loss: 0.647789 | Audio: 0.100762 | Latent: 5.470273 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  25%|███▎         | 2200/8750 [28:58<1:50:30,  1.01s/it, loss=0.6324, audio=0.0435, latent=5.8894, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2200/8750 | Loss: 0.647373 | Audio: 0.100600 | Latent: 5.467727 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  26%|███▍         | 2300/8750 [30:39<1:50:35,  1.03s/it, loss=0.5306, audio=0.0702, latent=4.6036, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2300/8750 | Loss: 0.646879 | Audio: 0.100362 | Latent: 5.465175 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  27%|███▌         | 2400/8750 [32:19<1:46:29,  1.01s/it, loss=0.6586, audio=0.1805, latent=4.7804, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2400/8750 | Loss: 0.647409 | Audio: 0.100744 | Latent: 5.466654 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  29%|███▋         | 2500/8750 [34:00<1:47:02,  1.03s/it, loss=0.6889, audio=0.1757, latent=5.1318, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2500/8750 | Loss: 0.647055 | Audio: 0.100790 | Latent: 5.462653 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  30%|███▊         | 2600/8750 [35:41<1:44:26,  1.02s/it, loss=0.5689, audio=0.0496, latent=5.1933, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2600/8750 | Loss: 0.647510 | Audio: 0.101133 | Latent: 5.463772 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  31%|████         | 2700/8750 [37:21<1:41:38,  1.01s/it, loss=0.5941, audio=0.0287, latent=5.6541, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2700/8750 | Loss: 0.647418 | Audio: 0.101426 | Latent: 5.459913 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  32%|████▏        | 2800/8750 [39:02<1:41:22,  1.02s/it, loss=0.5602, audio=0.0588, latent=5.0142, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2800/8750 | Loss: 0.647459 | Audio: 0.101558 | Latent: 5.459011 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  33%|████▎        | 2900/8750 [40:43<1:37:54,  1.00s/it, loss=0.5935, audio=0.0640, latent=5.2944, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 2900/8750 | Loss: 0.647235 | Audio: 0.101486 | Latent: 5.457493 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  34%|████▍        | 3000/8750 [42:24<1:35:26,  1.00it/s, loss=0.5925, audio=0.0659, latent=5.2665, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3000/8750 | Loss: 0.646861 | Audio: 0.101073 | Latent: 5.457874 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  35%|████▌        | 3100/8750 [44:05<1:34:47,  1.01s/it, loss=0.7040, audio=0.1779, latent=5.2609, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3100/8750 | Loss: 0.646545 | Audio: 0.100779 | Latent: 5.457660 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  37%|█████▍         | 3200/8750 [45:33<53:56,  1.72it/s, loss=0.5395, audio=0.0316, latent=5.0795, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3200/8750 | Loss: 0.646533 | Audio: 0.100883 | Latent: 5.456492 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  38%|█████▋         | 3300/8750 [46:32<53:54,  1.68it/s, loss=0.7960, audio=0.1928, latent=6.0318, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3300/8750 | Loss: 0.646622 | Audio: 0.101256 | Latent: 5.453666 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  39%|█████▊         | 3400/8750 [47:31<52:37,  1.69it/s, loss=0.7345, audio=0.1867, latent=5.4778, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3400/8750 | Loss: 0.646382 | Audio: 0.101295 | Latent: 5.450878 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  40%|██████         | 3500/8750 [48:30<51:13,  1.71it/s, loss=0.5113, audio=0.0409, latent=4.7036, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3500/8750 | Loss: 0.645955 | Audio: 0.101131 | Latent: 5.448240 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  41%|██████▏        | 3600/8750 [49:29<52:06,  1.65it/s, loss=0.5336, audio=0.0480, latent=4.8558, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3600/8750 | Loss: 0.645863 | Audio: 0.101189 | Latent: 5.446738 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  42%|██████▎        | 3700/8750 [50:28<51:00,  1.65it/s, loss=0.5307, audio=0.0426, latent=4.8817, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3700/8750 | Loss: 0.646049 | Audio: 0.101503 | Latent: 5.445468 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  43%|██████▌        | 3800/8750 [51:27<49:26,  1.67it/s, loss=0.5095, audio=0.0294, latent=4.8005, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3800/8750 | Loss: 0.645660 | Audio: 0.101299 | Latent: 5.443606 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  45%|██████▋        | 3900/8750 [52:27<47:25,  1.70it/s, loss=0.6144, audio=0.0517, latent=5.6264, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 3900/8750 | Loss: 0.645271 | Audio: 0.101074 | Latent: 5.441977 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  46%|██████▊        | 4000/8750 [53:27<47:23,  1.67it/s, loss=0.7490, audio=0.1999, latent=5.4916, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4000/8750 | Loss: 0.645115 | Audio: 0.101039 | Latent: 5.440765 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  47%|███████        | 4100/8750 [54:26<45:49,  1.69it/s, loss=0.7194, audio=0.1743, latent=5.4502, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4100/8750 | Loss: 0.645012 | Audio: 0.100991 | Latent: 5.440210 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  48%|███████▏       | 4200/8750 [55:25<44:36,  1.70it/s, loss=0.5615, audio=0.0209, latent=5.4060, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4200/8750 | Loss: 0.645404 | Audio: 0.101007 | Latent: 5.443973 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  49%|███████▎       | 4300/8750 [56:25<44:17,  1.67it/s, loss=0.5840, audio=0.0317, latent=5.5237, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4300/8750 | Loss: 0.645458 | Audio: 0.101032 | Latent: 5.444258 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  53%|███████▉       | 4600/8750 [59:23<40:13,  1.72it/s, loss=0.5664, audio=0.0630, latent=5.0337, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4600/8750 | Loss: 0.646055 | Audio: 0.101536 | Latent: 5.445185 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  54%|██████▉      | 4700/8750 [1:00:22<39:37,  1.70it/s, loss=0.5333, audio=0.0487, latent=4.8457, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4700/8750 | Loss: 0.645741 | Audio: 0.101342 | Latent: 5.443994 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  55%|███████▏     | 4800/8750 [1:01:21<38:44,  1.70it/s, loss=0.5870, audio=0.0254, latent=5.6168, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4800/8750 | Loss: 0.645679 | Audio: 0.101336 | Latent: 5.443428 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  56%|███████▎     | 4900/8750 [1:02:20<37:53,  1.69it/s, loss=0.5459, audio=0.0442, latent=5.0175, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 4900/8750 | Loss: 0.645365 | Audio: 0.101155 | Latent: 5.442099 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  57%|███████▍     | 5000/8750 [1:03:21<43:14,  1.45it/s, loss=0.5569, audio=0.0327, latent=5.2416, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5000/8750 | Loss: 0.645054 | Audio: 0.100950 | Latent: 5.441041 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  58%|███████▌     | 5100/8750 [1:04:22<36:49,  1.65it/s, loss=0.7266, audio=0.1763, latent=5.5030, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5100/8750 | Loss: 0.644843 | Audio: 0.100872 | Latent: 5.439707 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  59%|███████▋     | 5200/8750 [1:05:22<35:02,  1.69it/s, loss=0.5702, audio=0.0345, latent=5.3566, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5200/8750 | Loss: 0.644974 | Audio: 0.101015 | Latent: 5.439591 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  61%|███████▊     | 5300/8750 [1:06:20<33:29,  1.72it/s, loss=0.7029, audio=0.1791, latent=5.2384, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5300/8750 | Loss: 0.645203 | Audio: 0.101103 | Latent: 5.441003 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  62%|████████     | 5400/8750 [1:07:20<32:44,  1.71it/s, loss=0.6435, audio=0.0513, latent=5.9221, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5400/8750 | Loss: 0.645063 | Audio: 0.101121 | Latent: 5.439417 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  63%|████████▏    | 5500/8750 [1:08:18<31:53,  1.70it/s, loss=0.7167, audio=0.1980, latent=5.1865, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5500/8750 | Loss: 0.645151 | Audio: 0.101243 | Latent: 5.439083 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  64%|████████▎    | 5600/8750 [1:09:18<32:29,  1.62it/s, loss=0.5827, audio=0.0299, latent=5.5284, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5600/8750 | Loss: 0.644956 | Audio: 0.101265 | Latent: 5.436915 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  65%|████████▍    | 5700/8750 [1:10:19<32:07,  1.58it/s, loss=0.5911, audio=0.0275, latent=5.6357, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5700/8750 | Loss: 0.645006 | Audio: 0.101377 | Latent: 5.436291 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  66%|████████▌    | 5800/8750 [1:11:22<31:14,  1.57it/s, loss=0.5721, audio=0.0470, latent=5.2515, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5800/8750 | Loss: 0.645019 | Audio: 0.101301 | Latent: 5.437176 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  67%|████████▊    | 5900/8750 [1:12:25<29:03,  1.63it/s, loss=0.7662, audio=0.1776, latent=5.8851, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 5900/8750 | Loss: 0.645186 | Audio: 0.101503 | Latent: 5.436830 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  69%|████████▉    | 6000/8750 [1:13:27<28:56,  1.58it/s, loss=0.9053, audio=0.3276, latent=5.7767, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6000/8750 | Loss: 0.645194 | Audio: 0.101593 | Latent: 5.436006 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  69%|████████▉    | 6002/8750 [1:13:29<28:22,  1.61it/s, loss=0.8985, audio=0.3234, latent=5.7513, nans=1]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 40/40:  77%|█████████▉   | 6700/8750 [1:20:45<21:25,  1.60it/s, loss=0.5778, audio=0.0390, latent=5.3877, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6700/8750 | Loss: 0.644873 | Audio: 0.101602 | Latent: 5.432713 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  78%|██████████   | 6800/8750 [1:21:48<20:04,  1.62it/s, loss=0.6000, audio=0.0314, latent=5.6859, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6800/8750 | Loss: 0.645081 | Audio: 0.101768 | Latent: 5.433124 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  79%|██████████▎  | 6900/8750 [1:22:53<19:11,  1.61it/s, loss=0.7290, audio=0.1961, latent=5.3286, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 6900/8750 | Loss: 0.645149 | Audio: 0.101817 | Latent: 5.433317 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  80%|██████████▍  | 7000/8750 [1:23:56<18:35,  1.57it/s, loss=0.6898, audio=0.1921, latent=4.9767, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7000/8750 | Loss: 0.645310 | Audio: 0.102033 | Latent: 5.432767 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  81%|██████████▌  | 7100/8750 [1:24:59<17:14,  1.60it/s, loss=0.5567, audio=0.0739, latent=4.8286, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7100/8750 | Loss: 0.645250 | Audio: 0.102030 | Latent: 5.432207 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  82%|██████████▋  | 7200/8750 [1:26:01<16:58,  1.52it/s, loss=0.5359, audio=0.0412, latent=4.9477, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7200/8750 | Loss: 0.645233 | Audio: 0.102075 | Latent: 5.431579 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  83%|██████████▊  | 7300/8750 [1:27:06<15:18,  1.58it/s, loss=0.6219, audio=0.0314, latent=5.9048, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7300/8750 | Loss: 0.645048 | Audio: 0.101961 | Latent: 5.430876 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  85%|██████████▉  | 7400/8750 [1:28:08<14:13,  1.58it/s, loss=0.5079, audio=0.0309, latent=4.7703, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7400/8750 | Loss: 0.645075 | Audio: 0.101976 | Latent: 5.430983 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  86%|███████████▏ | 7500/8750 [1:29:16<13:47,  1.51it/s, loss=0.9130, audio=0.3254, latent=5.8760, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7500/8750 | Loss: 0.645034 | Audio: 0.102022 | Latent: 5.430113 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  87%|███████████▎ | 7600/8750 [1:30:22<12:47,  1.50it/s, loss=0.7530, audio=0.1702, latent=5.8276, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7600/8750 | Loss: 0.645134 | Audio: 0.102109 | Latent: 5.430246 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  88%|███████████▍ | 7700/8750 [1:31:29<11:35,  1.51it/s, loss=0.5890, audio=0.0424, latent=5.4661, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7700/8750 | Loss: 0.645029 | Audio: 0.102056 | Latent: 5.429729 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  89%|███████████▌ | 7800/8750 [1:32:34<09:40,  1.64it/s, loss=0.6835, audio=0.0405, latent=6.4294, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7800/8750 | Loss: 0.644789 | Audio: 0.101932 | Latent: 5.428569 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  90%|███████████▋ | 7900/8750 [1:33:37<09:08,  1.55it/s, loss=0.5813, audio=0.0329, latent=5.4835, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 7900/8750 | Loss: 0.644651 | Audio: 0.101875 | Latent: 5.427762 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  91%|███████████▉ | 8000/8750 [1:34:41<08:26,  1.48it/s, loss=0.6361, audio=0.0325, latent=6.0358, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8000/8750 | Loss: 0.644677 | Audio: 0.101856 | Latent: 5.428205 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  93%|████████████ | 8100/8750 [1:35:45<06:55,  1.57it/s, loss=0.7690, audio=0.1800, latent=5.8896, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8100/8750 | Loss: 0.644582 | Audio: 0.101720 | Latent: 5.428622 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  94%|████████████▏| 8200/8750 [1:36:48<05:48,  1.58it/s, loss=0.5264, audio=0.0467, latent=4.7976, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8200/8750 | Loss: 0.644406 | Audio: 0.101609 | Latent: 5.427969 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  95%|████████████▎| 8300/8750 [1:37:52<05:06,  1.47it/s, loss=0.5063, audio=0.0376, latent=4.6864, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8300/8750 | Loss: 0.644214 | Audio: 0.101498 | Latent: 5.427162 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  96%|████████████▍| 8400/8750 [1:38:56<03:41,  1.58it/s, loss=0.7041, audio=0.1773, latent=5.2682, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8400/8750 | Loss: 0.643983 | Audio: 0.101367 | Latent: 5.426151 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  97%|████████████▋| 8500/8750 [1:40:01<02:53,  1.44it/s, loss=0.6526, audio=0.0327, latent=6.1990, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8500/8750 | Loss: 0.643859 | Audio: 0.101192 | Latent: 5.426672 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  98%|████████████▊| 8600/8750 [1:41:11<01:37,  1.54it/s, loss=0.6048, audio=0.0402, latent=5.6462, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8600/8750 | Loss: 0.643757 | Audio: 0.101194 | Latent: 5.425629 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40:  99%|████████████▉| 8700/8750 [1:42:14<00:31,  1.60it/s, loss=0.7553, audio=0.1813, latent=5.7406, nans=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Step 8700/8750 | Loss: 0.643716 | Audio: 0.101219 | Latent: 5.424962 | NaNs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|█████████████| 8750/8750 [1:42:47<00:00,  1.42it/s, loss=0.5556, audio=0.0494, latent=5.0619, nans=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ Epoch had 1 NaN occurrences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 40/40: 100%|██████████████████| 1875/1875 [17:27<00:00,  1.79it/s, loss=0.5464, audio=0.0466, latent=4.9972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EPOCH 40/40 SUMMARY\n",
      "============================================================\n",
      "Train Loss:  0.643765 (Audio: 0.101314, Latent: 5.424511)\n",
      "Val Loss:    0.636827 (Audio: 0.098961, Latent: 5.378655)\n",
      "Learning Rate: 8.68e-06\n",
      "============================================================\n",
      "\n",
      "✅ NEW BEST MODEL! Val Loss: 0.636827\n",
      "\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TESTING BEST MODEL\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23812\\3221969232.py:971: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_ckpt = torch.load(cfg.best_model_path, map_location=cfg.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 41/40: 100%|██████████████████| 1875/1875 [17:21<00:00,  1.80it/s, loss=0.5522, audio=0.0314, latent=5.2080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL TEST RESULTS\n",
      "============================================================\n",
      "Test Loss:  0.639296\n",
      "  Audio Loss:  0.101333\n",
      "  Latent Loss: 5.379628\n",
      "============================================================\n",
      "\n",
      "Generating training curves...\n",
      "✓ Plot saved to: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/training_curves.png\n",
      "✓ Summary saved to: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/training_summary.json\n",
      "\n",
      "============================================================\n",
      "ALL FILES SAVED\n",
      "============================================================\n",
      "✓ Best model: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\n",
      "✓ Checkpoint: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model.pt\n",
      "✓ Training curves: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/training_curves.png\n",
      "✓ Summary: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/training_summary.json\n",
      "============================================================\n",
      "\n",
      "🎉 TRAINING PIPELINE COMPLETE! 🎉\n",
      "\n",
      "Next steps:\n",
      "1. Check training curves for convergence\n",
      "2. Use inference script to test on new audio\n",
      "3. Fine-tune hyperparameters if needed\n",
      "\n",
      "Good luck with your production model! 🚀\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Production-Ready Audio Effect Generator using DAC-VAE (No AudioTools)\n",
    "Path B: High-quality audio generation with stable training\n",
    "\n",
    "This implementation uses Descript Audio Codec (DAC) with VAE for:\n",
    "- Clean latent space manipulation\n",
    "- Stable gradient flow\n",
    "- Production-quality audio synthesis\n",
    "\n",
    "NO AUDIOTOOLS DEPENDENCY - Uses DAC encoder/decoder directly!\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DAC import (NO audiotools needed!)\n",
    "try:\n",
    "    import dac\n",
    "    print(\"✓ DAC library imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ DAC not installed. Run: pip install descript-audio-codec\")\n",
    "    exit(1)\n",
    "\n",
    "#############################################\n",
    "#                 CONFIG\n",
    "#############################################\n",
    "\n",
    "class CFG:\n",
    "    # Paths - KEEP YOUR PATHS\n",
    "    csv_path = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/25000_datapoints.csv\"\n",
    "    base_path = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA\"\n",
    "    checkpoint_path = f\"{base_path}/result_DAC/model.pt\"\n",
    "    best_model_path = f\"{base_path}/result_DAC/model_best.pt\"\n",
    "    plot_path = f\"{base_path}/result_DAC/training_curves.png\"\n",
    "    \n",
    "    # Create result directory if needed\n",
    "    os.makedirs(f\"{base_path}/result_DAC\", exist_ok=True)\n",
    "    \n",
    "    # Columns - SAME AS YOURS\n",
    "    audio_col_in = \"input_audio_path\"\n",
    "    audio_col_out = \"output_audio_path\"\n",
    "    text_col = \"prompt\"\n",
    "    \n",
    "    # Audio settings\n",
    "    sample_rate = 44100  # DAC uses 44.1kHz (better quality than 24kHz)\n",
    "    max_audio_length = 5 * 44100  # 5 seconds at 44.1kHz\n",
    "    \n",
    "    # DAC Model settings\n",
    "    dac_model_path = \"44khz\"  # Options: \"16khz\", \"24khz\", \"44khz\"\n",
    "    \n",
    "    # Training - OPTIMIZED FOR DAC\n",
    "    batch_size = 2  # Start small due to 44kHz\n",
    "    accumulation_steps = 4  # Effective batch = 8\n",
    "    epochs = 40  # More epochs for production quality\n",
    "    \n",
    "    # Learning rates - TUNED FOR DAC\n",
    "    lr_unet = 1e-5  # UNet learning rate\n",
    "    lr_text = 5e-7  # Text encoder learning rate (frozen mostly)\n",
    "    weight_decay = 0.01\n",
    "    grad_clip = 1.0\n",
    "    \n",
    "    # Loss weights\n",
    "    audio_loss_weight = 1.0  # Waveform reconstruction\n",
    "    latent_loss_weight = 0.1  # Latent space matching\n",
    "    \n",
    "    # Mixed precision\n",
    "    use_amp = True\n",
    "    \n",
    "    # Logging\n",
    "    log_interval = 100  # Print every 100 steps\n",
    "    \n",
    "    # UNet architecture - OPTIMIZED FOR DAC LATENTS\n",
    "    unet_channels = [64, 128, 256, 512]  # Deeper for better quality\n",
    "    text_dim = 768  # BERT hidden size\n",
    "    \n",
    "    # Data splits\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    \n",
    "    # Freezing options\n",
    "    freeze_text_encoder = True  # Set False to fine-tune BERT\n",
    "    freeze_dac = True  # ALWAYS keep True (don't touch DAC)\n",
    "    \n",
    "    # Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Num workers\n",
    "    num_workers = 0  # Windows compatibility\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DAC-VAE AUDIO EFFECT GENERATOR (No AudioTools)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {cfg.device}\")\n",
    "print(f\"Sample Rate: {cfg.sample_rate} Hz\")\n",
    "print(f\"Max Length: {cfg.max_audio_length / cfg.sample_rate:.1f} seconds\")\n",
    "print(f\"Batch Size: {cfg.batch_size} x {cfg.accumulation_steps} = {cfg.batch_size * cfg.accumulation_steps}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "#############################################\n",
    "#          LOAD DAC MODEL\n",
    "#############################################\n",
    "\n",
    "print(\"Loading DAC model...\")\n",
    "\n",
    "# Load from manually downloaded file (for offline use)\n",
    "dac_model_path = \"C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\"\n",
    "\n",
    "if not os.path.exists(dac_model_path):\n",
    "    print(f\"\\n❌ DAC model not found at: {dac_model_path}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MANUAL DOWNLOAD REQUIRED\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n📥 Download Instructions:\")\n",
    "    print(\"\\n1. Go to: https://github.com/descriptinc/descript-audio-codec/releases/tag/1.0.0\")\n",
    "    print(\"2. Download: weights_44khz_16kbps.pth (245 MB)\")\n",
    "    print(f\"3. Save to: {dac_model_path}\")\n",
    "    print(\"\\n💡 TIP: Use mobile hotspot if you have network/DNS issues!\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"✓ Loading from: {dac_model_path}\")\n",
    "dac_model = dac.DAC.load(dac_model_path)\n",
    "dac_model = dac_model.to(cfg.device)\n",
    "dac_model.eval()\n",
    "\n",
    "# Freeze DAC encoder and decoder (we only train UNet)\n",
    "for param in dac_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"✓ DAC model loaded and frozen\")\n",
    "\n",
    "# Get DAC latent dimensions\n",
    "with torch.no_grad():\n",
    "    dummy_audio = torch.randn(1, 1, cfg.sample_rate).to(cfg.device)\n",
    "    # Use encoder directly (no audiotools needed!)\n",
    "    z = dac_model.encoder(dummy_audio)\n",
    "    latent_channels = z.shape[1]\n",
    "    latent_time_reduction = dummy_audio.shape[-1] // z.shape[-1]\n",
    "    \n",
    "print(f\"✓ DAC Latent Channels: {latent_channels}\")\n",
    "print(f\"✓ Time Reduction Factor: {latent_time_reduction}x\")\n",
    "print()\n",
    "\n",
    "#############################################\n",
    "#      DATASET LOADING & PREPARATION\n",
    "#############################################\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.read_csv(cfg.csv_path)\n",
    "print(f\"Original dataset: {len(df)} samples\")\n",
    "\n",
    "# Fix paths - SAME AS YOUR CODE\n",
    "for col in [cfg.audio_col_in, cfg.audio_col_out]:\n",
    "    df[col] = df[col].apply(lambda p: os.path.join(cfg.base_path, str(p).replace('\\\\', '/')))\n",
    "\n",
    "# Validate files exist\n",
    "print(\"\\nValidating files...\")\n",
    "valid_indices = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Validating\"):\n",
    "    if os.path.exists(row[cfg.audio_col_in]) and os.path.exists(row[cfg.audio_col_out]):\n",
    "        valid_indices.append(idx)\n",
    "\n",
    "df = df.iloc[valid_indices].reset_index(drop=True)\n",
    "print(f\"✓ Valid samples: {len(df)}\")\n",
    "\n",
    "# Split dataset - SAME AS YOURS\n",
    "train_df, temp_df = train_test_split(df, test_size=(cfg.val_ratio + cfg.test_ratio), random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=cfg.test_ratio/(cfg.val_ratio + cfg.test_ratio), random_state=42)\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train:      {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "#############################################\n",
    "#      TOKENIZER\n",
    "#############################################\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"✓ Tokenizer loaded\\n\")\n",
    "\n",
    "#############################################\n",
    "#      DATASET CLASS\n",
    "#############################################\n",
    "\n",
    "class AudioEffectDataset(Dataset):\n",
    "    \"\"\"Dataset for audio effect generation using DAC (no audiotools)\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _load_and_process(self, path):\n",
    "        \"\"\"Load audio - returns torch tensor directly\"\"\"\n",
    "        # Load audio\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != cfg.sample_rate:\n",
    "            wav = torchaudio.functional.resample(wav, sr, cfg.sample_rate)\n",
    "        \n",
    "        # Convert to mono\n",
    "        if wav.size(0) > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # Trim or pad to max_length\n",
    "        if wav.size(1) > cfg.max_audio_length:\n",
    "            wav = wav[:, :cfg.max_audio_length]\n",
    "        elif wav.size(1) < cfg.max_audio_length:\n",
    "            wav = F.pad(wav, (0, cfg.max_audio_length - wav.size(1)))\n",
    "        \n",
    "        return wav\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.df.iloc[idx]\n",
    "            wav_in = self._load_and_process(row[cfg.audio_col_in])\n",
    "            wav_out = self._load_and_process(row[cfg.audio_col_out])\n",
    "            text = row[cfg.text_col]\n",
    "            \n",
    "            return wav_in, wav_out, text\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {e}\")\n",
    "            # Return zeros as fallback\n",
    "            return (\n",
    "                torch.zeros(1, cfg.max_audio_length),\n",
    "                torch.zeros(1, cfg.max_audio_length),\n",
    "                \"error loading audio\"\n",
    "            )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for batching\"\"\"\n",
    "    wav_in, wav_out, texts = zip(*batch)\n",
    "    \n",
    "    # Stack waveforms (already same length from dataset)\n",
    "    wav_in = torch.stack(wav_in)\n",
    "    wav_out = torch.stack(wav_out)\n",
    "    \n",
    "    # Tokenize texts\n",
    "    tokens = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return wav_in, wav_out, tokens.input_ids, tokens.attention_mask\n",
    "\n",
    "#############################################\n",
    "#      CREATE DATALOADERS\n",
    "#############################################\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING DATALOADERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_ds = AudioEffectDataset(train_df)\n",
    "val_ds = AudioEffectDataset(val_df)\n",
    "test_ds = AudioEffectDataset(test_df)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Batches per epoch:\")\n",
    "print(f\"  Train: {len(train_dl)} batches\")\n",
    "print(f\"  Val:   {len(val_dl)} batches\")\n",
    "print(f\"  Test:  {len(test_dl)} batches\")\n",
    "print()\n",
    "\n",
    "#############################################\n",
    "#      MODEL ARCHITECTURE\n",
    "#############################################\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention between audio latents and text embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dim, text_dim, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (audio_dim // n_heads) ** -0.5\n",
    "        \n",
    "        self.to_q = nn.Linear(audio_dim, audio_dim)\n",
    "        self.to_k = nn.Linear(text_dim, audio_dim)\n",
    "        self.to_v = nn.Linear(text_dim, audio_dim)\n",
    "        self.to_out = nn.Linear(audio_dim, audio_dim)\n",
    "        \n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        x: (B, C, T) - audio features\n",
    "        context: (B, S, D) - text embeddings\n",
    "        \"\"\"\n",
    "        B, C, T = x.shape\n",
    "        x_flat = rearrange(x, 'b c t -> b t c')\n",
    "        \n",
    "        q = self.to_q(x_flat)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        \n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h=self.n_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.n_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.n_heads)\n",
    "        \n",
    "        attn = torch.einsum('bhqd,bhkd->bhqk', q, k) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = torch.einsum('bhqk,bhvd->bhqd', attn, v)\n",
    "        out = rearrange(out, 'b h t d -> b t (h d)')\n",
    "        out = self.to_out(out)\n",
    "        \n",
    "        return rearrange(out, 'b t c -> b c t')\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with group normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.act(self.norm1(self.conv1(x)))\n",
    "        x = self.act(self.norm2(self.conv2(x)))\n",
    "        return x + residual\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"Downsampling block with optional cross-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, in_c, out_c, text_dim=768, use_attn=False):\n",
    "        super().__init__()\n",
    "        self.use_attn = use_attn\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_c, out_c, 3, padding=1)\n",
    "        self.res1 = ResidualBlock(out_c)\n",
    "        self.res2 = ResidualBlock(out_c)\n",
    "        \n",
    "        if use_attn:\n",
    "            self.attn = CrossAttention(out_c, text_dim)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(out_c, out_c, 4, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x, text_emb=None):\n",
    "        x = self.conv(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        \n",
    "        if self.use_attn and text_emb is not None:\n",
    "            x = x + self.attn(x, text_emb)\n",
    "        \n",
    "        skip = x\n",
    "        x = self.downsample(x)\n",
    "        return x, skip\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"Upsampling block with skip connections and optional cross-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, in_c, out_c, skip_c, text_dim=768, use_attn=False):\n",
    "        super().__init__()\n",
    "        self.use_attn = use_attn\n",
    "        \n",
    "        self.upsample = nn.ConvTranspose1d(in_c, out_c, 4, stride=2, padding=1)\n",
    "        self.conv = nn.Conv1d(out_c + skip_c, out_c, 3, padding=1)\n",
    "        self.res1 = ResidualBlock(out_c)\n",
    "        self.res2 = ResidualBlock(out_c)\n",
    "        \n",
    "        if use_attn:\n",
    "            self.attn = CrossAttention(out_c, text_dim)\n",
    "        \n",
    "    def forward(self, x, skip, text_emb=None):\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # Match temporal dimensions\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            x = F.interpolate(x, size=skip.size(-1), mode='linear', align_corners=False)\n",
    "        \n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        \n",
    "        if self.use_attn and text_emb is not None:\n",
    "            x = x + self.attn(x, text_emb)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class LatentUNet(nn.Module):\n",
    "    \"\"\"UNet for manipulating DAC latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_channels, channels, text_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_conv = nn.Conv1d(latent_channels, channels[0], 7, padding=3)\n",
    "        \n",
    "        # Encoder\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for i in range(len(channels) - 1):\n",
    "            use_attn = i >= 2  # Add attention in deeper layers\n",
    "            self.down_blocks.append(\n",
    "                DownBlock(channels[i], channels[i+1], text_dim, use_attn)\n",
    "            )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.mid_block1 = ResidualBlock(channels[-1])\n",
    "        self.mid_attn = CrossAttention(channels[-1], text_dim)\n",
    "        self.mid_block2 = ResidualBlock(channels[-1])\n",
    "        \n",
    "        # Decoder\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in range(len(channels) - 1, 0, -1):\n",
    "            use_attn = i >= 2\n",
    "            self.up_blocks.append(\n",
    "                UpBlock(\n",
    "                    in_c=channels[i],\n",
    "                    out_c=channels[i-1],\n",
    "                    skip_c=channels[i],\n",
    "                    text_dim=text_dim,\n",
    "                    use_attn=use_attn\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_conv = nn.Conv1d(channels[0], latent_channels, 7, padding=3)\n",
    "        \n",
    "    def forward(self, z, text_emb):\n",
    "        \"\"\"\n",
    "        z: (B, latent_channels, T) - DAC latents\n",
    "        text_emb: (B, S, text_dim) - text embeddings\n",
    "        \"\"\"\n",
    "        original_length = z.size(-1)\n",
    "        \n",
    "        x = self.input_conv(z)\n",
    "        \n",
    "        # Encoder path\n",
    "        skips = []\n",
    "        for down in self.down_blocks:\n",
    "            x, skip = down(x, text_emb)\n",
    "            skips.append(skip)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.mid_block1(x)\n",
    "        x = x + self.mid_attn(x, text_emb)\n",
    "        x = self.mid_block2(x)\n",
    "        \n",
    "        # Decoder path\n",
    "        for up in self.up_blocks:\n",
    "            skip = skips.pop()\n",
    "            x = up(x, skip, text_emb)\n",
    "        \n",
    "        # Output\n",
    "        x = self.output_conv(x)\n",
    "        \n",
    "        # Ensure output matches input length\n",
    "        if x.size(-1) != original_length:\n",
    "            x = F.interpolate(x, size=original_length, mode='linear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class AudioEffectModel(nn.Module):\n",
    "    \"\"\"Complete model: Text Encoder + UNet + DAC (no audiotools!)\"\"\"\n",
    "    \n",
    "    def __init__(self, dac_model, latent_channels, unet_channels, text_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder (BERT)\n",
    "        self.text_encoder = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Freeze/unfreeze text encoder based on config\n",
    "        if cfg.freeze_text_encoder:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Text encoder: FROZEN ❄️\")\n",
    "        else:\n",
    "            for param in self.text_encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Text encoder: TRAINABLE 🔥 (fine-tuning enabled)\")\n",
    "        \n",
    "        # DAC model (frozen)\n",
    "        self.dac = dac_model\n",
    "        \n",
    "        # UNet (trainable)\n",
    "        self.unet = LatentUNet(latent_channels, unet_channels, text_dim)\n",
    "        \n",
    "    def forward(self, wav_in, wav_out, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass with input validation (NO AUDIOTOOLS!)\n",
    "        \n",
    "        Returns:\n",
    "            wav_pred: Predicted output waveform\n",
    "            z_pred: Predicted latent\n",
    "            z_target: Target latent\n",
    "        \"\"\"\n",
    "        # Check for NaN in inputs\n",
    "        if torch.isnan(wav_in).any() or torch.isnan(wav_out).any():\n",
    "            return None, None, None\n",
    "        \n",
    "        # Encode text\n",
    "        text_output = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        text_emb = text_output.last_hidden_state  # (B, S, 768)\n",
    "        \n",
    "        # Encode audio to latents using DAC (NO AUDIOTOOLS - direct encoder call!)\n",
    "        with torch.no_grad():\n",
    "            z_in = self.dac.encoder(wav_in)\n",
    "            z_target = self.dac.encoder(wav_out)\n",
    "        \n",
    "        # Check for NaN in latents\n",
    "        if torch.isnan(z_in).any() or torch.isnan(z_target).any():\n",
    "            print(\"⚠️ NaN detected in DAC encoding\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Process with UNet\n",
    "        z_pred = self.unet(z_in, text_emb)\n",
    "        \n",
    "        # Check for NaN in prediction\n",
    "        if torch.isnan(z_pred).any():\n",
    "            print(\"⚠️ NaN detected in UNet output\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Decode latents to waveform (NO AUDIOTOOLS - direct decoder call!)\n",
    "        with torch.no_grad():\n",
    "            # Decode directly - decoder only needs the latents!\n",
    "            wav_pred = self.dac.decoder(z_pred)\n",
    "        \n",
    "        # Check for NaN in decoded audio\n",
    "        if torch.isnan(wav_pred).any():\n",
    "            print(\"⚠️ NaN detected in DAC decoding\")\n",
    "            return None, None, None\n",
    "        \n",
    "        return wav_pred, z_pred, z_target\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"Initialize weights with small values for stability\"\"\"\n",
    "    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        m.weight.data *= 0.1  # Scale down for stability\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight, gain=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.GroupNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "#############################################\n",
    "#     MODEL INITIALIZATION\n",
    "#############################################\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = AudioEffectModel(\n",
    "    dac_model=dac_model,\n",
    "    latent_channels=latent_channels,\n",
    "    unet_channels=cfg.unet_channels,\n",
    "    text_dim=cfg.text_dim\n",
    ").to(cfg.device)\n",
    "\n",
    "# Initialize UNet weights\n",
    "model.unet.apply(init_weights)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"UNet channels: {cfg.unet_channels}\")\n",
    "print(f\"Latent channels: {latent_channels}\")\n",
    "print()\n",
    "\n",
    "#############################################\n",
    "#     OPTIMIZER & LOSS\n",
    "#############################################\n",
    "\n",
    "# Optimizer based on freeze_text_encoder setting\n",
    "if cfg.freeze_text_encoder:\n",
    "    # Only optimize UNet\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.unet.parameters(),\n",
    "        lr=cfg.lr_unet,\n",
    "        weight_decay=cfg.weight_decay\n",
    "    )\n",
    "else:\n",
    "    # Optimize UNet + Text Encoder\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.unet.parameters(), \"lr\": cfg.lr_unet},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": cfg.lr_text},\n",
    "    ], weight_decay=cfg.weight_decay)\n",
    "\n",
    "# Cosine annealing scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=cfg.epochs * len(train_dl),\n",
    "    eta_min=cfg.lr_unet * 0.1\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "criterion_audio = nn.L1Loss()\n",
    "criterion_latent = nn.MSELoss()\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=cfg.use_amp)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Learning rate: {cfg.lr_unet}\")\n",
    "print(f\"Scheduler: CosineAnnealingLR\")\n",
    "print(f\"Loss: L1 (audio) + MSE (latent)\")\n",
    "print(f\"Mixed precision: {cfg.use_amp}\")\n",
    "print()\n",
    "\n",
    "#############################################\n",
    "#     TRAINING & VALIDATION FUNCTIONS\n",
    "#############################################\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_audio_loss = 0\n",
    "    total_latent_loss = 0\n",
    "    nan_count = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
    "    \n",
    "    for step, (wav_in, wav_out, ids, mask) in enumerate(pbar):\n",
    "        wav_in = wav_in.to(cfg.device)\n",
    "        wav_out = wav_out.to(cfg.device)\n",
    "        ids = ids.to(cfg.device)\n",
    "        mask = mask.to(cfg.device)\n",
    "        \n",
    "        # Check input\n",
    "        if torch.isnan(wav_in).any() or torch.isnan(wav_out).any():\n",
    "            print(f\"⚠️ NaN in input at step {step}, skipping...\")\n",
    "            nan_count += 1\n",
    "            continue\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=cfg.use_amp):\n",
    "            # Forward pass\n",
    "            wav_pred, z_pred, z_target = model(wav_in, wav_out, ids, mask)\n",
    "            \n",
    "            # Check for None (indicates NaN in forward pass)\n",
    "            if wav_pred is None:\n",
    "                nan_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Match lengths\n",
    "            if wav_pred.size(-1) != wav_out.size(-1):\n",
    "                min_len = min(wav_pred.size(-1), wav_out.size(-1))\n",
    "                wav_pred = wav_pred[..., :min_len]\n",
    "                wav_out = wav_out[..., :min_len]\n",
    "            \n",
    "            if z_pred.size(-1) != z_target.size(-1):\n",
    "                min_len = min(z_pred.size(-1), z_target.size(-1))\n",
    "                z_pred = z_pred[..., :min_len]\n",
    "                z_target = z_target[..., :min_len]\n",
    "            \n",
    "            # Compute losses\n",
    "            audio_loss = criterion_audio(wav_pred, wav_out)\n",
    "            latent_loss = criterion_latent(z_pred, z_target)\n",
    "            \n",
    "            loss = (cfg.audio_loss_weight * audio_loss + \n",
    "                   cfg.latent_loss_weight * latent_loss)\n",
    "            \n",
    "            # Scale for gradient accumulation\n",
    "            loss = loss / cfg.accumulation_steps\n",
    "        \n",
    "        # Check loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"⚠️ NaN/Inf loss at step {step}, skipping...\")\n",
    "            nan_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Optimizer step (with gradient accumulation)\n",
    "        if (step + 1) % cfg.accumulation_steps == 0:\n",
    "            # Unscale gradients\n",
    "            scaler.unscale_(optimizer)\n",
    "            \n",
    "            # Clip gradients\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                cfg.grad_clip\n",
    "            )\n",
    "            \n",
    "            # Check gradient norm\n",
    "            if torch.isnan(grad_norm) or torch.isinf(grad_norm) or grad_norm > 100:\n",
    "                print(f\"⚠️ Bad gradient (norm={grad_norm:.2f}) at step {step}, skipping...\")\n",
    "                optimizer.zero_grad()\n",
    "                scaler.update()\n",
    "                nan_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Update weights\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_loss += loss.item() * cfg.accumulation_steps\n",
    "        total_audio_loss += audio_loss.item()\n",
    "        total_latent_loss += latent_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item() * cfg.accumulation_steps:.4f}',\n",
    "            'audio': f'{audio_loss.item():.4f}',\n",
    "            'latent': f'{latent_loss.item():.4f}',\n",
    "            'nans': nan_count\n",
    "        })\n",
    "        \n",
    "        # Log every N steps\n",
    "        if (step + 1) % cfg.log_interval == 0:\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            print(f\"\\n  Step {step+1}/{len(dataloader)} | \"\n",
    "                  f\"Loss: {avg_loss:.6f} | \"\n",
    "                  f\"Audio: {total_audio_loss/(step+1):.6f} | \"\n",
    "                  f\"Latent: {total_latent_loss/(step+1):.6f} | \"\n",
    "                  f\"NaNs: {nan_count}\")\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f\"\\n⚠️ Epoch had {nan_count} NaN occurrences\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_audio_loss = total_audio_loss / len(dataloader)\n",
    "    avg_latent_loss = total_latent_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_audio_loss, avg_latent_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(model, dataloader, epoch):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_audio_loss = 0\n",
    "    total_latent_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Validation {epoch+1}/{cfg.epochs}\")\n",
    "    \n",
    "    for wav_in, wav_out, ids, mask in pbar:\n",
    "        wav_in = wav_in.to(cfg.device)\n",
    "        wav_out = wav_out.to(cfg.device)\n",
    "        ids = ids.to(cfg.device)\n",
    "        mask = mask.to(cfg.device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=cfg.use_amp):\n",
    "            wav_pred, z_pred, z_target = model(wav_in, wav_out, ids, mask)\n",
    "            \n",
    "            if wav_pred is None:\n",
    "                continue\n",
    "            \n",
    "            # Match lengths\n",
    "            if wav_pred.size(-1) != wav_out.size(-1):\n",
    "                min_len = min(wav_pred.size(-1), wav_out.size(-1))\n",
    "                wav_pred = wav_pred[..., :min_len]\n",
    "                wav_out = wav_out[..., :min_len]\n",
    "            \n",
    "            if z_pred.size(-1) != z_target.size(-1):\n",
    "                min_len = min(z_pred.size(-1), z_target.size(-1))\n",
    "                z_pred = z_pred[..., :min_len]\n",
    "                z_target = z_target[..., :min_len]\n",
    "            \n",
    "            audio_loss = criterion_audio(wav_pred, wav_out)\n",
    "            latent_loss = criterion_latent(z_pred, z_target)\n",
    "            \n",
    "            loss = (cfg.audio_loss_weight * audio_loss + \n",
    "                   cfg.latent_loss_weight * latent_loss)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_audio_loss += audio_loss.item()\n",
    "        total_latent_loss += latent_loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'audio': f'{audio_loss.item():.4f}',\n",
    "            'latent': f'{latent_loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_audio_loss = total_audio_loss / len(dataloader)\n",
    "    avg_latent_loss = total_latent_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, avg_audio_loss, avg_latent_loss\n",
    "\n",
    "#############################################\n",
    "#     TRAINING LOOP\n",
    "#############################################\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total epochs: {cfg.epochs}\")\n",
    "print(f\"Steps per epoch: {len(train_dl)}\")\n",
    "print(f\"Validation every epoch\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_audio_losses = []\n",
    "train_latent_losses = []\n",
    "val_audio_losses = []\n",
    "val_latent_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "# Resume from checkpoint if exists\n",
    "if os.path.exists(cfg.checkpoint_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    ckpt = torch.load(cfg.checkpoint_path, map_location=cfg.device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    scheduler.load_state_dict(ckpt['scheduler'])\n",
    "    scaler.load_state_dict(ckpt['scaler'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    train_losses = ckpt.get('train_losses', [])\n",
    "    val_losses = ckpt.get('val_losses', [])\n",
    "    train_audio_losses = ckpt.get('train_audio_losses', [])\n",
    "    train_latent_losses = ckpt.get('train_latent_losses', [])\n",
    "    val_audio_losses = ckpt.get('val_audio_losses', [])\n",
    "    val_latent_losses = ckpt.get('val_latent_losses', [])\n",
    "    best_val_loss = ckpt.get('best_val_loss', float('inf'))\n",
    "    print(f\"✓ Resumed from epoch {start_epoch}\")\n",
    "    print()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, cfg.epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch+1}/{cfg.epochs}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_audio, train_latent = train_epoch(\n",
    "        model, train_dl, optimizer, scheduler, scaler, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_audio, val_latent = validate_epoch(\n",
    "        model, val_dl, epoch\n",
    "    )\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_audio_losses.append(train_audio)\n",
    "    train_latent_losses.append(train_latent)\n",
    "    val_audio_losses.append(val_audio)\n",
    "    val_latent_losses.append(val_latent)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch+1}/{cfg.epochs} SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train Loss:  {train_loss:.6f} (Audio: {train_audio:.6f}, Latent: {train_latent:.6f})\")\n",
    "    print(f\"Val Loss:    {val_loss:.6f} (Audio: {val_audio:.6f}, Latent: {val_latent:.6f})\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'scaler': scaler.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_audio_losses': train_audio_losses,\n",
    "        'train_latent_losses': train_latent_losses,\n",
    "        'val_audio_losses': val_audio_losses,\n",
    "        'val_latent_losses': val_latent_losses,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': {\n",
    "            'latent_channels': latent_channels,\n",
    "            'unet_channels': cfg.unet_channels,\n",
    "            'text_dim': cfg.text_dim,\n",
    "            'sample_rate': cfg.sample_rate\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, cfg.checkpoint_path)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "            'config': checkpoint['config']\n",
    "        }, cfg.best_model_path)\n",
    "        print(f\"✅ NEW BEST MODEL! Val Loss: {best_val_loss:.6f}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "#############################################\n",
    "#     TEST SET EVALUATION\n",
    "#############################################\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING BEST MODEL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Load best model\n",
    "best_ckpt = torch.load(cfg.best_model_path, map_location=cfg.device)\n",
    "model.load_state_dict(best_ckpt['model'])\n",
    "print(f\"Loaded best model from epoch {best_ckpt['epoch']}\")\n",
    "\n",
    "# Test\n",
    "test_loss, test_audio, test_latent = validate_epoch(model, test_dl, cfg.epochs)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Loss:  {test_loss:.6f}\")\n",
    "print(f\"  Audio Loss:  {test_audio:.6f}\")\n",
    "print(f\"  Latent Loss: {test_latent:.6f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "#############################################\n",
    "#     PLOT TRAINING CURVES\n",
    "#############################################\n",
    "\n",
    "print(\"Generating training curves...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Total loss\n",
    "ax = axes[0, 0]\n",
    "epochs_range = range(len(train_losses))\n",
    "ax.plot(epochs_range, train_losses, 'b-', label='Train', linewidth=2, marker='o', markersize=4)\n",
    "ax.plot(epochs_range, val_losses, 'r-', label='Val', linewidth=2, marker='s', markersize=4)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Total Loss', fontsize=12)\n",
    "ax.set_title('Total Loss (Audio + Latent)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Audio loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs_range, train_audio_losses, 'b-', label='Train', linewidth=2, marker='o', markersize=4)\n",
    "ax.plot(epochs_range, val_audio_losses, 'r-', label='Val', linewidth=2, marker='s', markersize=4)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Audio Loss (L1)', fontsize=12)\n",
    "ax.set_title('Audio Reconstruction Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Latent loss\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs_range, train_latent_losses, 'b-', label='Train', linewidth=2, marker='o', markersize=4)\n",
    "ax.plot(epochs_range, val_latent_losses, 'r-', label='Val', linewidth=2, marker='s', markersize=4)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Latent Loss (MSE)', fontsize=12)\n",
    "ax.set_title('Latent Space Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Generalization gap\n",
    "ax = axes[1, 1]\n",
    "gap = [v - t for t, v in zip(train_losses, val_losses)]\n",
    "ax.plot(epochs_range, gap, 'g-', label='Val - Train', linewidth=2, marker='d', markersize=4)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss Gap', fontsize=12)\n",
    "ax.set_title('Generalization Gap (Val - Train)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Plot saved to: {cfg.plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "#############################################\n",
    "#     SAVE SUMMARY\n",
    "#############################################\n",
    "\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(train_df),\n",
    "        'val_samples': len(val_df),\n",
    "        'test_samples': len(test_df)\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': cfg.epochs,\n",
    "        'batch_size': cfg.batch_size,\n",
    "        'accumulation_steps': cfg.accumulation_steps,\n",
    "        'effective_batch_size': cfg.batch_size * cfg.accumulation_steps\n",
    "    },\n",
    "    'model': {\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'latent_channels': latent_channels,\n",
    "        'unet_channels': cfg.unet_channels\n",
    "    },\n",
    "    'results': {\n",
    "        'best_train_loss': float(min(train_losses)),\n",
    "        'best_val_loss': float(best_val_loss),\n",
    "        'test_loss': float(test_loss),\n",
    "        'test_audio_loss': float(test_audio),\n",
    "        'test_latent_loss': float(test_latent)\n",
    "    },\n",
    "    'config': {\n",
    "        'sample_rate': cfg.sample_rate,\n",
    "        'max_audio_length': cfg.max_audio_length,\n",
    "        'lr_unet': cfg.lr_unet,\n",
    "        'audio_loss_weight': cfg.audio_loss_weight,\n",
    "        'latent_loss_weight': cfg.latent_loss_weight\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = f\"{cfg.base_path}/result_DAC/training_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL FILES SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Best model: {cfg.best_model_path}\")\n",
    "print(f\"✓ Checkpoint: {cfg.checkpoint_path}\")\n",
    "print(f\"✓ Training curves: {cfg.plot_path}\")\n",
    "print(f\"✓ Summary: {summary_path}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n🎉 TRAINING PIPELINE COMPLETE! 🎉\\n\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Check training curves for convergence\")\n",
    "print(\"2. Use inference script to test on new audio\")\n",
    "print(\"3. Fine-tune hyperparameters if needed\")\n",
    "print(\"\\nGood luck with your production model! 🚀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2c6f5-c63a-4c81-b8b5-39d56c0d0aca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbed36b6-630b-4b5b-93f6-d8ba15655565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DAC library imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --input INPUT --output OUTPUT --prompt PROMPT [--dac-model DAC_MODEL]\n",
      "                             [--device DEVICE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --input, --output, --prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Inference Script for DAC-VAE Audio Effect Generator\n",
    "Handles variable-length audio and all common formats\n",
    "\n",
    "FEATURES:\n",
    "- No audiotools dependency\n",
    "- Handles any audio length (not limited to 5 seconds)\n",
    "- Supports all common formats (.wav, .mp3, .flac, .ogg, .m4a, etc.)\n",
    "- Uses soundfile for robust cross-platform compatibility\n",
    "- Chunk-based processing for long audio files\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from einops import rearrange\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DAC import\n",
    "try:\n",
    "    import dac\n",
    "    print(\"✓ DAC library imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ DAC not installed. Run: pip install descript-audio-codec\")\n",
    "    exit(1)\n",
    "\n",
    "#############################################\n",
    "#     MODEL ARCHITECTURE (SAME AS TRAINING)\n",
    "#############################################\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention between audio latents and text embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dim, text_dim, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (audio_dim // n_heads) ** -0.5\n",
    "        self.to_q = nn.Linear(audio_dim, audio_dim)\n",
    "        self.to_k = nn.Linear(text_dim, audio_dim)\n",
    "        self.to_v = nn.Linear(text_dim, audio_dim)\n",
    "        self.to_out = nn.Linear(audio_dim, audio_dim)\n",
    "        \n",
    "    def forward(self, x, context):\n",
    "        B, C, T = x.shape\n",
    "        x_flat = rearrange(x, 'b c t -> b t c')\n",
    "        q = self.to_q(x_flat)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        q = rearrange(q, 'b t (h d) -> b h t d', h=self.n_heads)\n",
    "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.n_heads)\n",
    "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.n_heads)\n",
    "        attn = torch.einsum('bhqd,bhkd->bhqk', q, k) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhqk,bhvd->bhqd', attn, v)\n",
    "        out = rearrange(out, 'b h t d -> b t (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return rearrange(out, 'b t c -> b c t')\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with group normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.act(self.norm1(self.conv1(x)))\n",
    "        x = self.act(self.norm2(self.conv2(x)))\n",
    "        return x + residual\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"Downsampling block with optional cross-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, in_c, out_c, text_dim=768, use_attn=False):\n",
    "        super().__init__()\n",
    "        self.use_attn = use_attn\n",
    "        self.conv = nn.Conv1d(in_c, out_c, 3, padding=1)\n",
    "        self.res1 = ResidualBlock(out_c)\n",
    "        self.res2 = ResidualBlock(out_c)\n",
    "        if use_attn:\n",
    "            self.attn = CrossAttention(out_c, text_dim)\n",
    "        self.downsample = nn.Conv1d(out_c, out_c, 4, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x, text_emb=None):\n",
    "        x = self.conv(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        if self.use_attn and text_emb is not None:\n",
    "            x = x + self.attn(x, text_emb)\n",
    "        skip = x\n",
    "        x = self.downsample(x)\n",
    "        return x, skip\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"Upsampling block with skip connections and optional cross-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, in_c, out_c, skip_c, text_dim=768, use_attn=False):\n",
    "        super().__init__()\n",
    "        self.use_attn = use_attn\n",
    "        self.upsample = nn.ConvTranspose1d(in_c, out_c, 4, stride=2, padding=1)\n",
    "        self.conv = nn.Conv1d(out_c + skip_c, out_c, 3, padding=1)\n",
    "        self.res1 = ResidualBlock(out_c)\n",
    "        self.res2 = ResidualBlock(out_c)\n",
    "        if use_attn:\n",
    "            self.attn = CrossAttention(out_c, text_dim)\n",
    "        \n",
    "    def forward(self, x, skip, text_emb=None):\n",
    "        x = self.upsample(x)\n",
    "        if x.size(-1) != skip.size(-1):\n",
    "            x = F.interpolate(x, size=skip.size(-1), mode='linear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        if self.use_attn and text_emb is not None:\n",
    "            x = x + self.attn(x, text_emb)\n",
    "        return x\n",
    "\n",
    "class LatentUNet(nn.Module):\n",
    "    \"\"\"UNet for manipulating DAC latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_channels, channels, text_dim=768):\n",
    "        super().__init__()\n",
    "        self.input_conv = nn.Conv1d(latent_channels, channels[0], 7, padding=3)\n",
    "        \n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for i in range(len(channels) - 1):\n",
    "            use_attn = i >= 2\n",
    "            self.down_blocks.append(DownBlock(channels[i], channels[i+1], text_dim, use_attn))\n",
    "        \n",
    "        self.mid_block1 = ResidualBlock(channels[-1])\n",
    "        self.mid_attn = CrossAttention(channels[-1], text_dim)\n",
    "        self.mid_block2 = ResidualBlock(channels[-1])\n",
    "        \n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in range(len(channels) - 1, 0, -1):\n",
    "            use_attn = i >= 2\n",
    "            self.up_blocks.append(\n",
    "                UpBlock(channels[i], channels[i-1], channels[i], text_dim, use_attn)\n",
    "            )\n",
    "        \n",
    "        self.output_conv = nn.Conv1d(channels[0], latent_channels, 7, padding=3)\n",
    "        \n",
    "    def forward(self, z, text_emb):\n",
    "        original_length = z.size(-1)\n",
    "        x = self.input_conv(z)\n",
    "        \n",
    "        skips = []\n",
    "        for down in self.down_blocks:\n",
    "            x, skip = down(x, text_emb)\n",
    "            skips.append(skip)\n",
    "        \n",
    "        x = self.mid_block1(x)\n",
    "        x = x + self.mid_attn(x, text_emb)\n",
    "        x = self.mid_block2(x)\n",
    "        \n",
    "        for up in self.up_blocks:\n",
    "            skip = skips.pop()\n",
    "            x = up(x, skip, text_emb)\n",
    "        \n",
    "        x = self.output_conv(x)\n",
    "        \n",
    "        if x.size(-1) != original_length:\n",
    "            x = F.interpolate(x, size=original_length, mode='linear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class AudioEffectModel(nn.Module):\n",
    "    \"\"\"Complete model: Text Encoder + UNet + DAC\"\"\"\n",
    "    \n",
    "    def __init__(self, dac_model, latent_channels, unet_channels, text_dim):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dac = dac_model\n",
    "        self.unet = LatentUNet(latent_channels, unet_channels, text_dim)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(self, wav_in, prompt, sample_rate):\n",
    "        \"\"\"Generate audio with effect applied\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # Ensure correct shape\n",
    "        if wav_in.dim() == 2:\n",
    "            wav_in = wav_in.unsqueeze(1)\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        tokens = tokenizer(\n",
    "            [prompt],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(wav_in.device)\n",
    "        \n",
    "        # Encode text\n",
    "        text_output = self.text_encoder(\n",
    "            input_ids=tokens.input_ids,\n",
    "            attention_mask=tokens.attention_mask\n",
    "        )\n",
    "        text_emb = text_output.last_hidden_state\n",
    "        \n",
    "        # Encode audio to latents\n",
    "        z_in = self.dac.encoder(wav_in)\n",
    "        \n",
    "        # Process with UNet\n",
    "        z_out = self.unet(z_in, text_emb)\n",
    "        \n",
    "        # Decode to waveform\n",
    "        wav_out = self.dac.decoder(z_out)\n",
    "        \n",
    "        return wav_out\n",
    "\n",
    "#############################################\n",
    "#     ENHANCED INFERENCE CLASS\n",
    "#############################################\n",
    "class AudioEffectInference:\n",
    "    def __init__(self, model_path, dac_model_path=None, device='cuda'):\n",
    "        \"\"\"Initialize inference pipeline\"\"\"\n",
    "        self.device = device if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"LOADING MODEL FOR INFERENCE (NO AUDIOTOOLS)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        print(f\"Loading checkpoint from: {model_path}\")\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # Get config\n",
    "        config = ckpt['config']\n",
    "        self.sample_rate = config['sample_rate']\n",
    "        latent_channels = config['latent_channels']\n",
    "        unet_channels = config['unet_channels']\n",
    "        text_dim = config['text_dim']\n",
    "        \n",
    "        print(f\"✓ Sample rate: {self.sample_rate} Hz\")\n",
    "        print(f\"✓ Latent channels: {latent_channels}\")\n",
    "        print(f\"✓ UNet channels: {unet_channels}\")\n",
    "        \n",
    "        # Load DAC model\n",
    "        if dac_model_path is None:\n",
    "            dac_model_path = \"C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\"\n",
    "        \n",
    "        print(f\"Loading DAC model from: {dac_model_path}\")\n",
    "        \n",
    "        if not os.path.exists(dac_model_path):\n",
    "            print(f\"\\n❌ DAC model not found at: {dac_model_path}\")\n",
    "            print(\"Please download it first!\")\n",
    "            exit(1)\n",
    "        \n",
    "        self.dac_model = dac.DAC.load(dac_model_path)\n",
    "        self.dac_model = self.dac_model.to(self.device)\n",
    "        self.dac_model.eval()\n",
    "        print(\"✓ DAC model loaded\")\n",
    "        \n",
    "        # Create model\n",
    "        self.model = AudioEffectModel(\n",
    "            dac_model=self.dac_model,\n",
    "            latent_channels=latent_channels,\n",
    "            unet_channels=unet_channels,\n",
    "            text_dim=text_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Load weights\n",
    "        self.model.load_state_dict(ckpt['model'])\n",
    "        self.model.eval()\n",
    "        print(\"✓ Model weights loaded\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        global tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        print(\"✓ Tokenizer loaded\")\n",
    "        \n",
    "        print(f\"✓ Device: {self.device}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    def process_audio(self, input_path, output_path, prompt):\n",
    "        \"\"\"\n",
    "        Process audio file with effect (Simple version - works for most cases)\n",
    "        \"\"\"\n",
    "        import soundfile as sf\n",
    "        \n",
    "        print(f\"Processing: {input_path}\")\n",
    "        print(f\"Effect: '{prompt}'\")\n",
    "        \n",
    "        # Load audio using soundfile (handles all formats)\n",
    "        wav, sr = sf.read(input_path)\n",
    "        wav = torch.from_numpy(wav).float()\n",
    "        \n",
    "        # Ensure correct shape: (channels, samples)\n",
    "        if wav.dim() == 1:\n",
    "            wav = wav.unsqueeze(0)\n",
    "        elif wav.dim() == 2 and wav.size(0) > wav.size(1):\n",
    "            wav = wav.t()\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            print(f\"Resampling from {sr} Hz to {self.sample_rate} Hz\")\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.sample_rate)\n",
    "        \n",
    "        # Convert to mono\n",
    "        if wav.size(0) > 1:\n",
    "            print(\"Converting to mono\")\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # Add batch dimension and move to device\n",
    "        wav = wav.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        print(f\"Input shape: {wav.shape}\")\n",
    "        print(\"Generating...\")\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            wav_out = self.model.generate(wav, prompt, self.sample_rate)\n",
    "        \n",
    "        # Move to CPU and remove batch dimension\n",
    "        wav_out = wav_out.squeeze(0).cpu()\n",
    "        \n",
    "        # Match original length\n",
    "        current_length = wav_out.size(-1)\n",
    "        target_length = wav.squeeze(0).size(-1)\n",
    "        \n",
    "        if current_length != target_length:\n",
    "            print(f\"Adjusting length: {current_length} -> {target_length}\")\n",
    "            if current_length > target_length:\n",
    "                wav_out = wav_out[..., :target_length]\n",
    "            else:\n",
    "                wav_out = F.pad(wav_out, (0, target_length - current_length))\n",
    "        \n",
    "        print(f\"Output shape: {wav_out.shape}\")\n",
    "        \n",
    "        # Save using soundfile\n",
    "        wav_out_np = wav_out.squeeze(0).numpy()\n",
    "        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
    "        sf.write(output_path, wav_out_np, self.sample_rate)\n",
    "        \n",
    "        print(f\"✓ Saved to: {output_path}\\n\")\n",
    "    \n",
    "    def batch_process(self, input_dir, output_dir, prompt):\n",
    "        \"\"\"Process all audio files in a directory\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # ✅ Support all common audio formats\n",
    "        audio_extensions = ['.wav', '.mp3', '.flac', '.ogg', '.m4a', '.aac', '.wma', '.aiff']\n",
    "        audio_files = [\n",
    "            f for f in os.listdir(input_dir)\n",
    "            if os.path.splitext(f)[1].lower() in audio_extensions\n",
    "        ]\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"❌ No audio files found in {input_dir}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(audio_files)} audio files\")\n",
    "        print(f\"Effect: '{prompt}'\\n\")\n",
    "        \n",
    "        for i, filename in enumerate(audio_files, 1):\n",
    "            print(f\"[{i}/{len(audio_files)}] Processing: {filename}\")\n",
    "            \n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            name, ext = os.path.splitext(filename)\n",
    "            output_filename = f\"{name}_processed.wav\"  # ✅ Always save as WAV\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            try:\n",
    "                self.process_audio(input_path, output_path, prompt)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {filename}: {e}\\n\")\n",
    "        \n",
    "        print(f\"\\n✅ Batch processing complete!\")\n",
    "        print(f\"   Processed: {len(audio_files)} files\")\n",
    "        print(f\"   Output directory: {output_dir}\")\n",
    "    \n",
    "\n",
    "\n",
    "#############################################\n",
    "#     MAIN FUNCTION\n",
    "#############################################\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Enhanced Audio Effect Generator Inference',\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Examples:\n",
    "  # Single file (any length, any format)\n",
    "  python inference_enhanced.py --model model_best.pt --input song.mp3 --output song_rain.wav --prompt \"add rain sounds\"\n",
    "  \n",
    "  # Batch processing\n",
    "  python inference_enhanced.py --model model_best.pt --input audio_folder/ --output results/ --prompt \"add birds chirping\"\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('--model', type=str, required=True,\n",
    "                       help='Path to trained model checkpoint (.pt file)')\n",
    "    parser.add_argument('--input', type=str, required=True,\n",
    "                       help='Input audio file or directory')\n",
    "    parser.add_argument('--output', type=str, required=True,\n",
    "                       help='Output audio file or directory')\n",
    "    parser.add_argument('--prompt', type=str, required=True,\n",
    "                       help='Effect description (e.g., \"add rain sounds\")')\n",
    "    parser.add_argument('--dac-model', type=str, default=None,\n",
    "                       help='Path to DAC model weights')\n",
    "    parser.add_argument('--device', type=str, default='cuda',\n",
    "                       help='Device to use (cuda or cpu)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize inference\n",
    "    try:\n",
    "        inference = AudioEffectInference(args.model, args.dac_model, args.device)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Process\n",
    "    if os.path.isfile(args.input):\n",
    "        inference.process_audio(args.input, args.output, args.prompt)\n",
    "    elif os.path.isdir(args.input):\n",
    "        inference.batch_process(args.input, args.output, args.prompt)\n",
    "    else:\n",
    "        print(f\"❌ Error: {args.input} is not a valid file or directory\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b00aab-f719-43bc-ae10-520e19701aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENHANCED AUDIO EFFECT INFERENCE\n",
      "============================================================\n",
      "Loading checkpoint from: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23812\\1575757296.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample rate: 44100 Hz\n",
      "✓ Max chunk length: 5.0s\n",
      "✓ Latent channels: 128\n",
      "✓ UNet channels: [64, 128, 256, 512]\n",
      "Loading DAC model from: C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\n",
      "✓ DAC model loaded\n",
      "✓ Model weights loaded\n",
      "✓ Tokenizer loaded\n",
      "✓ Device: cuda\n",
      "============================================================\n",
      "\n",
      "Processing: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/22.wav\n",
      "Effect: 'add dogs sounds'\n",
      "  Original: 22050 Hz, 5.00s\n",
      "  Resampling: 22050 Hz → 44100 Hz\n",
      "  Input shape: torch.Size([1, 220500])\n",
      "  Output shape: torch.Size([1, 220500])\n",
      "✓ Saved to: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/22_dogs.wav\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --input INPUT --output OUTPUT --prompt PROMPT [--dac-model DAC_MODEL]\n",
      "                             [--device DEVICE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --input, --output, --prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example usage (can be run directly or via command line)\n",
    "    \n",
    "    # For direct usage in script:\n",
    "    \n",
    "    model_path = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\"\n",
    "    input_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/22.wav\"\n",
    "    output_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/22_dogs.wav\"\n",
    "    effect_prompt = \"add dogs sounds\"\n",
    "    \n",
    "    inference = AudioEffectInference(model_path, device='cuda')\n",
    "    inference.process_audio(input_audio, output_audio, effect_prompt)\n",
    "    \n",
    "    # For command line usage:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fafcd4b3-02ca-4638-a518-8d89f4367a1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING MODEL FOR INFERENCE (NO AUDIOTOOLS)\n",
      "============================================================\n",
      "Loading checkpoint from: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23812\\2242883022.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample rate: 44100 Hz\n",
      "✓ Latent channels: 128\n",
      "✓ UNet channels: [64, 128, 256, 512]\n",
      "Loading DAC model from: C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\n",
      "✓ DAC model loaded\n",
      "✓ Model weights loaded\n",
      "✓ Tokenizer loaded\n",
      "✓ Device: cuda\n",
      "============================================================\n",
      "\n",
      "Processing: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/zahra.wav\n",
      "Effect: 'add dogs sounds'\n",
      "Resampling from 16000 Hz to 44100 Hz\n",
      "Input shape: torch.Size([1, 1, 239808])\n",
      "Generating...\n",
      "Adjusting length: 239616 -> 239808\n",
      "Output shape: torch.Size([1, 239808])\n",
      "✓ Saved to: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/zahra_dogs.wav\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --input INPUT --output OUTPUT --prompt PROMPT [--dac-model DAC_MODEL]\n",
      "                             [--device DEVICE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --input, --output, --prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example usage (can be run directly or via command line)\n",
    "    \n",
    "    # For direct usage in script:\n",
    "    \n",
    "    model_path = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\"\n",
    "    input_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/zahra.wav\"\n",
    "    output_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/zahra_dogs.wav\"\n",
    "    effect_prompt = \"add dogs sounds\"\n",
    "    \n",
    "    inference = AudioEffectInference(model_path, device='cuda')\n",
    "    inference.process_audio(input_audio, output_audio, effect_prompt)\n",
    "    \n",
    "    # For command line usage:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43b09e6a-76aa-474a-a2ca-cc09bdf65a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING MODEL FOR INFERENCE (NO AUDIOTOOLS)\n",
      "============================================================\n",
      "Loading checkpoint from: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23812\\2242883022.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample rate: 44100 Hz\n",
      "✓ Latent channels: 128\n",
      "✓ UNet channels: [64, 128, 256, 512]\n",
      "Loading DAC model from: C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\n",
      "✓ DAC model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016909E4EDB0>: Failed to resolve \\'huggingface.co\\' ([Errno 11002] getaddrinfo failed)\"))'), '(Request ID: dac7b205-75cf-439c-9a97-736d5ad35abd)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016A1E199280>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d396ccd6-ccbd-4248-92ae-5b4e3a76aa0c)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000016909E5A030>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f3a094ca-3886-4563-bc65-9c24847e23d8)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model weights loaded\n",
      "✓ Tokenizer loaded\n",
      "✓ Device: cuda\n",
      "============================================================\n",
      "\n",
      "Processing: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/arabic_XBmfzfHL.wav\n",
      "Effect: 'add dogs sounds'\n",
      "Resampling from 48000 Hz to 44100 Hz\n",
      "Converting to mono\n",
      "Input shape: torch.Size([1, 1, 219618])\n",
      "Generating...\n",
      "Adjusting length: 219136 -> 219618\n",
      "Output shape: torch.Size([1, 219618])\n",
      "✓ Saved to: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/arabic_XBmfzfHL_dogs.wav\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --input INPUT --output OUTPUT --prompt PROMPT [--dac-model DAC_MODEL]\n",
      "                             [--device DEVICE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --input, --output, --prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example usage (can be run directly or via command line)\n",
    "    \n",
    "    # For direct usage in script:\n",
    "    \n",
    "    model_path = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\"\n",
    "    input_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/arabic_XBmfzfHL.wav\"\n",
    "    output_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/arabic_XBmfzfHL_dogs.wav\"\n",
    "    effect_prompt = \"add dogs sounds\"\n",
    "    \n",
    "    inference = AudioEffectInference(model_path, device='cuda')\n",
    "    inference.process_audio(input_audio, output_audio, effect_prompt)\n",
    "    \n",
    "    # For command line usage:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37253d22-b8f8-49b4-bae7-7cd9208fe2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "459e2064-db3c-43aa-800f-c2ac22c2dcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING MODEL FOR INFERENCE (NO AUDIOTOOLS)\n",
      "============================================================\n",
      "Loading checkpoint from: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23812\\2242883022.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample rate: 44100 Hz\n",
      "✓ Latent channels: 128\n",
      "✓ UNet channels: [64, 128, 256, 512]\n",
      "Loading DAC model from: C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\n",
      "✓ DAC model loaded\n",
      "✓ Model weights loaded\n",
      "✓ Tokenizer loaded\n",
      "✓ Device: cuda\n",
      "============================================================\n",
      "\n",
      "Processing: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/english_OJCIvTNk.wav\n",
      "Effect: 'add dogs sounds'\n",
      "Resampling from 48000 Hz to 44100 Hz\n",
      "Converting to mono\n",
      "Input shape: torch.Size([1, 1, 213003])\n",
      "Generating...\n",
      "Adjusting length: 212992 -> 213003\n",
      "Output shape: torch.Size([1, 213003])\n",
      "✓ Saved to: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/english_OJCIvTNk_dogs.wav\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --input INPUT --output OUTPUT --prompt PROMPT [--dac-model DAC_MODEL]\n",
      "                             [--device DEVICE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --input, --output, --prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example usage (can be run directly or via command line)\n",
    "    \n",
    "    # For direct usage in script:\n",
    "    \n",
    "    model_path = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\"\n",
    "    input_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/english_OJCIvTNk.wav\"\n",
    "    output_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/english_OJCIvTNk_dogs.wav\"\n",
    "    effect_prompt = \"add dogs sounds\"\n",
    "    \n",
    "    inference = AudioEffectInference(model_path, device='cuda')\n",
    "    inference.process_audio(input_audio, output_audio, effect_prompt)\n",
    "    \n",
    "    # For command line usage:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ac61cd2-68fd-4a51-9c37-e4df2504b174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING MODEL FOR INFERENCE (NO AUDIOTOOLS)\n",
      "============================================================\n",
      "Loading checkpoint from: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23812\\2242883022.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample rate: 44100 Hz\n",
      "✓ Latent channels: 128\n",
      "✓ UNet channels: [64, 128, 256, 512]\n",
      "Loading DAC model from: C:/Users/user/.cache/dac/weights_44khz_16kbps.pth\n",
      "✓ DAC model loaded\n",
      "✓ Model weights loaded\n",
      "✓ Tokenizer loaded\n",
      "✓ Device: cuda\n",
      "============================================================\n",
      "\n",
      "Processing: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/french.wav\n",
      "Effect: 'add dogs sounds'\n",
      "Resampling from 16000 Hz to 44100 Hz\n",
      "Input shape: torch.Size([1, 1, 233634])\n",
      "Generating...\n",
      "Adjusting length: 233472 -> 233634\n",
      "Output shape: torch.Size([1, 233634])\n",
      "✓ Saved to: C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/french_dogs.wav\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --input INPUT --output OUTPUT --prompt PROMPT [--dac-model DAC_MODEL]\n",
      "                             [--device DEVICE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --input, --output, --prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example usage (can be run directly or via command line)\n",
    "    \n",
    "    # For direct usage in script:\n",
    "    \n",
    "    model_path = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/model_best.pt\"\n",
    "    input_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/french.wav\"\n",
    "    output_audio = \"C:/Users/user/Desktop/yassine/EchoMind/data/NEW_DATA/result_DAC/inference_40/french_dogs.wav\"\n",
    "    effect_prompt = \"add dogs sounds\"\n",
    "    \n",
    "    inference = AudioEffectInference(model_path, device='cuda')\n",
    "    inference.process_audio(input_audio, output_audio, effect_prompt)\n",
    "    \n",
    "    # For command line usage:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62786d87-304d-410b-96a1-6171145ae472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
