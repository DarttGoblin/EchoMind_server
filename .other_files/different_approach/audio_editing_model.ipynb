{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33096659-019f-4342-9847-ce782ad05b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Dataset ----------------\n",
    "class AudioEditingDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Load .npy files\n",
    "        prompt = np.load(row['prompt_embedding'])         # shape (num_tokens, prompt_dim)\n",
    "        input_audio = np.load(row['input_embedding'])    # shape (audio_dim,)\n",
    "        output_audio = np.load(row['output_embedding'])  # shape (audio_dim,)\n",
    "\n",
    "        # Pool prompt if needed\n",
    "        if prompt.ndim > 1:\n",
    "            prompt = prompt.mean(axis=0)\n",
    "\n",
    "        # Flatten audio if multi-frame\n",
    "        if input_audio.ndim > 1:\n",
    "            input_audio = input_audio.mean(axis=0)\n",
    "        if output_audio.ndim > 1:\n",
    "            output_audio = output_audio.mean(axis=0)\n",
    "\n",
    "        # Convert to tensors\n",
    "        prompt = torch.tensor(prompt, dtype=torch.float32)\n",
    "        input_audio = torch.tensor(input_audio, dtype=torch.float32)\n",
    "        output_audio = torch.tensor(output_audio, dtype=torch.float32)\n",
    "\n",
    "        return prompt, input_audio, output_audio\n",
    "\n",
    "# ---------------- Transformer Model ----------------\n",
    "class AudioEditingTransformer(nn.Module):\n",
    "    def __init__(self, prompt_dim=1024, audio_dim=128, embedding_dim=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project prompt and audio embeddings to same dimension\n",
    "        self.proj_prompt = nn.Linear(prompt_dim, embedding_dim)\n",
    "        self.proj_audio = nn.Linear(audio_dim, embedding_dim)\n",
    "\n",
    "        # Positional encoding for sequence length 2\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 2, embedding_dim))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output MLP to predict edited audio embedding\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, audio_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, prompt_embedding, input_audio_embedding):\n",
    "        # Project to same embedding_dim\n",
    "        prompt_embedding = self.proj_prompt(prompt_embedding)\n",
    "        input_audio_embedding = self.proj_audio(input_audio_embedding)\n",
    "\n",
    "        # Stack into sequence length 2\n",
    "        x = torch.stack([prompt_embedding, input_audio_embedding], dim=1)  # (batch_size, 2, embedding_dim)\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Use last token (audio) to predict output\n",
    "        out = self.mlp(x[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# ---------------- Training Loop ----------------\n",
    "def train_model(csv_file, embedding_dim=128, batch_size=64, epochs=20, lr=1e-4, device='cuda'):\n",
    "    dataset = AudioEditingDataset(csv_file)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = AudioEditingTransformer(embedding_dim=embedding_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for prompt, input_audio, output_audio in dataloader:\n",
    "            prompt = prompt.to(device)\n",
    "            input_audio = input_audio.to(device)\n",
    "            output_audio = output_audio.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(prompt, input_audio)\n",
    "            loss = criterion(pred, output_audio)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example inference\n",
    "# -----------------------------\n",
    "def infer(model, prompt_embedding, input_audio_embedding, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prompt_embedding = prompt_embedding.to(device).float().unsqueeze(0)\n",
    "        input_audio_embedding = input_audio_embedding.to(device).float().unsqueeze(0)\n",
    "        pred_embedding = model(prompt_embedding, input_audio_embedding)\n",
    "    return pred_embedding.squeeze(0)  # Return embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f029a7-297c-4dec-8122-1c67afb1398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Shape: (128,)\n",
      "First 10 values: [175.   8. 147.  97. 215.  76.  77. 130. 148. 175.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "emb = np.load('../embeddings/input_data_embeddings/1.npy', allow_pickle=True)\n",
    "\n",
    "print(type(emb))\n",
    "print(\"Shape:\", emb.shape)\n",
    "print(\"First 10 values:\", emb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c18931b1-ae40-4f69-88b6-9de39c41bba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.load(\"../embeddings/prompts_embeddings/embeddings_snowfall.npy\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "777be838-c4e1-41b8-8b55-b422170c32dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 96.7417\n",
      "Epoch 2/20 - Loss: 2.1898\n",
      "Epoch 3/20 - Loss: 86.3750\n",
      "Epoch 4/20 - Loss: 39.4520\n",
      "Epoch 5/20 - Loss: 19.0071\n",
      "Epoch 6/20 - Loss: 26.9399\n",
      "Epoch 7/20 - Loss: 54.3680\n",
      "Epoch 8/20 - Loss: 1.1060\n",
      "Epoch 9/20 - Loss: 14.2896\n",
      "Epoch 10/20 - Loss: 27.4440\n",
      "Epoch 11/20 - Loss: 14.5823\n",
      "Epoch 12/20 - Loss: 1.0573\n",
      "Epoch 13/20 - Loss: 7.1401\n",
      "Epoch 14/20 - Loss: 4.3344\n",
      "Epoch 15/20 - Loss: 32.1758\n",
      "Epoch 16/20 - Loss: 25.2243\n",
      "Epoch 17/20 - Loss: 40.1536\n",
      "Epoch 18/20 - Loss: 55.1921\n",
      "Epoch 19/20 - Loss: 1.1630\n",
      "Epoch 20/20 - Loss: 29.1605\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2. Train the model\n",
    "csv_file = '../50000_datapoint.csv'\n",
    "embedding_dim = 512  # Set this to match your embedding size\n",
    "model = train_model(csv_file, embedding_dim=embedding_dim, batch_size=64, epochs=20, lr=1e-4, device=device)\n",
    "\n",
    "# 3. Save the trained model\n",
    "torch.save(model.state_dict(), 'audio_editing_transformer.pth')\n",
    "\n",
    "# # 4. Load the trained model later\n",
    "# model_loaded = AudioEditingTransformer(embedding_dim=embedding_dim).to(device)\n",
    "# model_loaded.load_state_dict(torch.load('audio_editing_transformer.pth'))\n",
    "\n",
    "# # 5. Example inference with new embeddings\n",
    "# # Suppose you have torch tensors: prompt_emb, input_audio_emb\n",
    "# # They should have shape [embedding_dim]\n",
    "# predicted_output_emb = infer(model_loaded, prompt_emb, input_audio_emb, device=device)\n",
    "\n",
    "# # 6. Use predicted_output_emb to reconstruct audio via your decoder / vocoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a4c2a-c649-4ae1-95b5-3cee25fcaf96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
